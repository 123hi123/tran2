"""
æ‰‹èªè¾¨è­˜æ¨¡å‹è¨“ç·´è…³æœ¬ v1
åŠŸèƒ½ï¼š
1. è¼‰å…¥é è™•ç†å¾Œçš„è¨“ç·´è³‡æ–™é›†
2. å»ºç«‹å’Œé…ç½®æ·±åº¦å­¸ç¿’æ¨¡å‹
3. è¨“ç·´æ¨¡å‹
4. å„²å­˜è¨“ç·´å¥½çš„æ¨¡å‹
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import joblib
import os
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

class SignLanguageGRU(nn.Module):
    """æ‰‹èªè¾¨è­˜GRUæ¨¡å‹"""
    
    def __init__(self, input_size, hidden_size=128, num_layers=2, num_classes=10, dropout=0.3):
        super(SignLanguageGRU, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # GRUå±¤
        self.gru = nn.GRU(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0,
            bidirectional=True
        )
        
        # å…¨é€£æ¥å±¤
        self.fc1 = nn.Linear(hidden_size * 2, hidden_size)  # *2 å› ç‚ºé›™å‘GRU
        self.dropout = nn.Dropout(dropout)
        self.fc2 = nn.Linear(hidden_size, num_classes)
        self.relu = nn.ReLU()
        
    def forward(self, x):
        # x shape: (batch_size, sequence_length, input_size)
        gru_out, _ = self.gru(x)
        
        # å–æœ€å¾Œä¸€å€‹æ™‚é–“æ­¥çš„è¼¸å‡º
        last_output = gru_out[:, -1, :]  # (batch_size, hidden_size * 2)
        
        # é€šéå…¨é€£æ¥å±¤
        out = self.fc1(last_output)
        out = self.relu(out)
        out = self.dropout(out)
        out = self.fc2(out)
        
        return out

class SignLanguageTrainer:
    def __init__(self, data_folder="v1/processed_data", model_folder="v1/models"):
        self.data_folder = data_folder
        self.model_folder = model_folder
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = None
        self.label_encoder = None
        
        print(f"ä½¿ç”¨è¨­å‚™: {self.device}")
        
    def setup_directories(self):
        """å»ºç«‹æ¨¡å‹è¼¸å‡ºè³‡æ–™å¤¾"""
        os.makedirs(self.model_folder, exist_ok=True)
        print(f"æ¨¡å‹è³‡æ–™å¤¾å·²å»ºç«‹: {self.model_folder}")
    
    def load_data(self):
        """è¼‰å…¥é è™•ç†å¾Œçš„è³‡æ–™"""
        train_path = os.path.join(self.data_folder, "train_dataset.csv")
        encoder_path = os.path.join(self.data_folder, "label_encoder.pkl")
        
        if not os.path.exists(train_path):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°è¨“ç·´è³‡æ–™é›†: {train_path}")
        if not os.path.exists(encoder_path):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ¨™ç±¤ç·¨ç¢¼å™¨: {encoder_path}")
        
        # è¼‰å…¥è¨“ç·´è³‡æ–™
        train_data = pd.read_csv(train_path)
        print(f"è¼‰å…¥è¨“ç·´è³‡æ–™: {train_data.shape}")
        
        # è¼‰å…¥æ¨™ç±¤ç·¨ç¢¼å™¨
        self.label_encoder = joblib.load(encoder_path)
        print(f"é¡åˆ¥æ•¸é‡: {len(self.label_encoder.classes_)}")
        print(f"é¡åˆ¥: {list(self.label_encoder.classes_)}")
        
        return train_data
    
    def analyze_sequence_lengths(self, data):
        """åˆ†æå„é¡åˆ¥çš„åºåˆ—é•·åº¦åˆ†å¸ƒ"""
        print("\nğŸ“Š åºåˆ—é•·åº¦åˆ†æ:")
        print("-" * 50)
        
        length_stats = {}
        for sign_language in data['sign_language'].unique():
            class_data = data[data['sign_language'] == sign_language]
            length = len(class_data)
            length_stats[sign_language] = length
            
            print(f"{sign_language:<15}: {length:>3} å¹€")
        
        # çµ±è¨ˆæ‘˜è¦
        lengths = list(length_stats.values())
        print("-" * 50)
        print(f"{'å¹³å‡é•·åº¦':<15}: {np.mean(lengths):>6.1f} å¹€")
        print(f"{'æœ€çŸ­å‹•ä½œ':<15}: {np.min(lengths):>6} å¹€")
        print(f"{'æœ€é•·å‹•ä½œ':<15}: {np.max(lengths):>6} å¹€")
        print(f"{'æ¨™æº–å·®':<15}: {np.std(lengths):>6.1f} å¹€")
        
        # å»ºè­°æœ€ä½³sequence_length
        recommended_length = int(np.percentile(lengths, 70))  # 70%åˆ†ä½æ•¸
        print(f"{'å»ºè­°åºåˆ—é•·åº¦':<15}: {recommended_length:>6} å¹€ (è¦†è“‹70%å‹•ä½œ)")
        print("-" * 50)
        
        return length_stats, recommended_length

    def prepare_sequences(self, data, sequence_length=30):
        """
        æº–å‚™åºåˆ—è³‡æ–™ - å°‡CSVè¡¨æ ¼è½‰æ›ç‚ºGRUæ¨¡å‹æ‰€éœ€çš„æ™‚åºåºåˆ—æ ¼å¼
        
        ç›®æ¨™: 
        - è¼¸å…¥: CSVè¡¨æ ¼ (æ¯è¡Œ=ä¸€å¹€, åŒ…å«163ç¶­ç‰¹å¾µ)
        - è¼¸å‡º: 3Då¼µé‡ (æ¨£æœ¬æ•¸, æ™‚é–“æ­¥, ç‰¹å¾µç¶­åº¦)
        
        Args:
            data: é è™•ç†å¾Œçš„è¨“ç·´è³‡æ–™ (DataFrame)
            sequence_length: æ¯å€‹åºåˆ—çš„æ™‚é–“æ­¥æ•¸ (é è¨­30å¹€)
            
        Returns:
            sequences: å½¢ç‹€ç‚º (æ¨£æœ¬æ•¸, sequence_length, ç‰¹å¾µç¶­åº¦) çš„3Då¼µé‡
            labels: å½¢ç‹€ç‚º (æ¨£æœ¬æ•¸,) çš„æ¨™ç±¤é™£åˆ—
        """
        
        print(f"é–‹å§‹æº–å‚™åºåˆ—è³‡æ–™ï¼Œç›®æ¨™åºåˆ—é•·åº¦: {sequence_length}")
        
        # æ­¥é©Ÿ1: æå–ç‰¹å¾µæ¬„ä½ (ç§»é™¤æ¨™ç±¤æ¬„ä½ï¼Œåªä¿ç•™åº§æ¨™ç‰¹å¾µ)
        feature_cols = [col for col in data.columns 
                       if col not in ['sign_language', 'sign_language_encoded']]
        
        print(f"ç‰¹å¾µæ¬„ä½æ•¸é‡: {len(feature_cols)}")
        print(f"ç‰¹å¾µæ¬„ä½åŒ…å«: frame + å§¿æ…‹åº§æ¨™ + å·¦æ‰‹åº§æ¨™ + å³æ‰‹åº§æ¨™")
        
        # æ­¥é©Ÿ2: åˆå§‹åŒ–åºåˆ—å®¹å™¨
        sequences = []  # å„²å­˜æ‰€æœ‰æ™‚åºåºåˆ—
        labels = []     # å„²å­˜å°æ‡‰çš„æ¨™ç±¤
        
        # æ­¥é©Ÿ3: æŒ‰æ‰‹èªé¡åˆ¥åˆ†çµ„è™•ç†
        unique_classes = data['sign_language'].unique()
        print(f"è™•ç† {len(unique_classes)} å€‹æ‰‹èªé¡åˆ¥: {list(unique_classes)}")
        
        for sign_language in unique_classes:
            # æå–è©²é¡åˆ¥çš„æ‰€æœ‰å¹€è³‡æ–™
            class_data = data[data['sign_language'] == sign_language]
            num_frames = len(class_data)
            
            print(f"\nè™•ç†é¡åˆ¥ '{sign_language}': {num_frames} å¹€")
            
            # æƒ…æ³A: è³‡æ–™å……è¶³ï¼Œä½¿ç”¨æ»‘å‹•çª—å£å‰µå»ºå¤šå€‹åºåˆ—
            if num_frames >= sequence_length:
                num_sequences = num_frames - sequence_length + 1
                print(f"  â†’ ä½¿ç”¨æ»‘å‹•çª—å£å‰µå»º {num_sequences} å€‹åºåˆ—")
                
                for i in range(num_sequences):
                    # æå–é€£çºŒçš„sequence_lengthå¹€ä½œç‚ºä¸€å€‹åºåˆ—
                    seq = class_data.iloc[i:i+sequence_length][feature_cols].values
                    sequences.append(seq)
                    labels.append(class_data.iloc[i]['sign_language_encoded'])
                    
            # æƒ…æ³B: è³‡æ–™ä¸è¶³ï¼Œä½¿ç”¨å¡«å……ç­–ç•¥
            else:
                print(f"  â†’ è³‡æ–™ä¸è¶³ï¼Œé€²è¡Œå¡«å…… ({num_frames} â†’ {sequence_length} å¹€)")
                
                seq_data = class_data[feature_cols].values
                
                if len(seq_data) < sequence_length:
                    # è¨ˆç®—éœ€è¦å¡«å……çš„å¹€æ•¸
                    padding_needed = sequence_length - len(seq_data)
                    
                    # ä½¿ç”¨æœ€å¾Œä¸€å¹€é‡è¤‡å¡«å……
                    last_frame = seq_data[-1:] if len(seq_data) > 0 else np.zeros((1, len(feature_cols)))
                    padding = np.repeat(last_frame, padding_needed, axis=0)
                    seq_data = np.vstack([seq_data, padding])
                    
                    print(f"  â†’ é‡è¤‡æœ€å¾Œä¸€å¹€ {padding_needed} æ¬¡é€²è¡Œå¡«å……")
                
                sequences.append(seq_data)
                labels.append(class_data.iloc[0]['sign_language_encoded'])
        
        # æ­¥é©Ÿ4: è½‰æ›ç‚ºNumPyé™£åˆ—
        sequences = np.array(sequences, dtype=np.float32)
        labels = np.array(labels, dtype=np.int64)
        
        # æ­¥é©Ÿ5: é¡¯ç¤ºæœ€çµ‚çµæœ
        print(f"\nâœ… åºåˆ—æº–å‚™å®Œæˆ!")
        print(f"åºåˆ—å½¢ç‹€: {sequences.shape}")
        print(f"  - è¨“ç·´æ¨£æœ¬æ•¸: {sequences.shape[0]}")
        print(f"  - åºåˆ—é•·åº¦(æ™‚é–“æ­¥): {sequences.shape[1]}")
        print(f"  - ç‰¹å¾µç¶­åº¦: {sequences.shape[2]}")
        print(f"æ¨™ç±¤å½¢ç‹€: {labels.shape}")
        
        return sequences, labels
    
    def create_model(self, input_size, num_classes):
        """å‰µå»ºæ¨¡å‹"""
        self.model = SignLanguageGRU(
            input_size=input_size,
            hidden_size=128,
            num_layers=2,
            num_classes=num_classes,
            dropout=0.3
        ).to(self.device)
        
        print(f"æ¨¡å‹åƒæ•¸æ•¸é‡: {sum(p.numel() for p in self.model.parameters()):,}")
        print(f"å¯è¨“ç·´åƒæ•¸æ•¸é‡: {sum(p.numel() for p in self.model.parameters() if p.requires_grad):,}")
        
        return self.model
    
    def train_model(self, X_train, y_train, epochs=100, batch_size=32, learning_rate=0.001):
        """è¨“ç·´æ¨¡å‹"""
        # å‰µå»ºè³‡æ–™è¼‰å…¥å™¨
        train_dataset = TensorDataset(
            torch.FloatTensor(X_train),
            torch.LongTensor(y_train)
        )
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        
        # å®šç¾©æå¤±å‡½æ•¸å’Œå„ªåŒ–å™¨
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(self.model.parameters(), lr=learning_rate, weight_decay=1e-5)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=10, factor=0.5)
        
        # è¨“ç·´è¨˜éŒ„
        train_losses = []
        train_accuracies = []
        
        print(f"\né–‹å§‹è¨“ç·´ - Epochs: {epochs}, Batch Size: {batch_size}, Learning Rate: {learning_rate}")
        print("-" * 70)
        
        for epoch in range(epochs):
            self.model.train()
            total_loss = 0
            correct = 0
            total = 0
            
            for batch_X, batch_y in train_loader:
                batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)
                
                # å‰å‘å‚³æ’­
                optimizer.zero_grad()
                outputs = self.model(batch_X)
                loss = criterion(outputs, batch_y)
                
                # åå‘å‚³æ’­
                loss.backward()
                optimizer.step()
                
                # çµ±è¨ˆ
                total_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                total += batch_y.size(0)
                correct += (predicted == batch_y).sum().item()
            
            # è¨ˆç®—å¹³å‡æå¤±å’Œæº–ç¢ºç‡
            avg_loss = total_loss / len(train_loader)
            accuracy = 100 * correct / total
            
            train_losses.append(avg_loss)
            train_accuracies.append(accuracy)
            
            # å­¸ç¿’ç‡èª¿æ•´
            scheduler.step(avg_loss)
            
            # æ¯10å€‹epochæˆ–æœ€å¾Œä¸€å€‹epochæ‰“å°çµæœ
            if (epoch + 1) % 10 == 0 or epoch == epochs - 1:
                print(f"Epoch [{epoch+1:3d}/{epochs}] | "
                      f"Loss: {avg_loss:.4f} | "
                      f"Accuracy: {accuracy:.2f}% | "
                      f"LR: {optimizer.param_groups[0]['lr']:.6f}")
        
        print("-" * 70)
        print("è¨“ç·´å®Œæˆ!")
        
        return train_losses, train_accuracies
    
    def save_model(self, train_losses, train_accuracies):
        """å„²å­˜æ¨¡å‹å’Œè¨“ç·´è¨˜éŒ„"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # å„²å­˜æ¨¡å‹
        model_path = os.path.join(self.model_folder, f"sign_language_gru_v1_{timestamp}.pth")
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'model_config': {
                'input_size': self.model.gru.input_size,
                'hidden_size': self.model.hidden_size,
                'num_layers': self.model.num_layers,
                'num_classes': len(self.label_encoder.classes_)
            },
            'label_encoder': self.label_encoder,
            'train_losses': train_losses,
            'train_accuracies': train_accuracies
        }, model_path)
        
        print(f"æ¨¡å‹å·²å„²å­˜: {model_path}")
        
        # å„²å­˜æœ€æ–°æ¨¡å‹çš„è·¯å¾‘ï¼ˆç”¨æ–¼æ¸¬è©¦ï¼‰
        latest_model_path = os.path.join(self.model_folder, "latest_model.pth")
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'model_config': {
                'input_size': self.model.gru.input_size,
                'hidden_size': self.model.hidden_size,
                'num_layers': self.model.num_layers,
                'num_classes': len(self.label_encoder.classes_)
            },
            'label_encoder': self.label_encoder,
            'train_losses': train_losses,
            'train_accuracies': train_accuracies
        }, latest_model_path)
        
        print(f"æœ€æ–°æ¨¡å‹é€£çµå·²æ›´æ–°: {latest_model_path}")
        
        return model_path
    
    def plot_training_curves(self, train_losses, train_accuracies):
        """ç¹ªè£½è¨“ç·´æ›²ç·š"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
        
        # æå¤±æ›²ç·š
        ax1.plot(train_losses, 'b-', label='Training Loss')
        ax1.set_title('Training Loss')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.legend()
        ax1.grid(True)
        
        # æº–ç¢ºç‡æ›²ç·š
        ax2.plot(train_accuracies, 'r-', label='Training Accuracy')
        ax2.set_title('Training Accuracy')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Accuracy (%)')
        ax2.legend()
        ax2.grid(True)
        
        plt.tight_layout()
        
        # å„²å­˜åœ–è¡¨
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        plot_path = os.path.join(self.model_folder, f"training_curves_{timestamp}.png")
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        print(f"è¨“ç·´æ›²ç·šå·²å„²å­˜: {plot_path}")
        
        plt.show()
    
    def run_training(self, epochs=100, batch_size=32, learning_rate=0.001, sequence_length=30):
        """åŸ·è¡Œå®Œæ•´çš„è¨“ç·´æµç¨‹"""
        print("=" * 70)
        print("æ‰‹èªè¾¨è­˜æ¨¡å‹è¨“ç·´ v1")
        print("=" * 70)
        
        # 1. å»ºç«‹è¼¸å‡ºè³‡æ–™å¤¾
        self.setup_directories()
        
        # 2. è¼‰å…¥è³‡æ–™
        print("\næ­¥é©Ÿ 1: è¼‰å…¥è³‡æ–™...")
        train_data = self.load_data()
        
        # 3. åˆ†æåºåˆ—é•·åº¦åˆ†å¸ƒ
        print("\næ­¥é©Ÿ 2a: åˆ†æåºåˆ—é•·åº¦åˆ†å¸ƒ...")
        length_stats, recommended_length = self.analyze_sequence_lengths(train_data)
        
        # æ ¹æ“šåˆ†æçµæœèª¿æ•´sequence_lengthï¼ˆå¯é¸ï¼‰
        if sequence_length == 20:  # å¦‚æœä½¿ç”¨é è¨­å€¼ï¼Œè€ƒæ…®ä½¿ç”¨å»ºè­°å€¼
            print(f"\nğŸ’¡ å»ºè­°ä½¿ç”¨åºåˆ—é•·åº¦: {recommended_length} (ç•¶å‰ä½¿ç”¨: {sequence_length})")
        
        # 4. æº–å‚™åºåˆ—è³‡æ–™
        print("\næ­¥é©Ÿ 2b: æº–å‚™åºåˆ—è³‡æ–™...")
        X_train, y_train = self.prepare_sequences(train_data, sequence_length)
        
        # 5. å‰µå»ºæ¨¡å‹
        print("\næ­¥é©Ÿ 3: å‰µå»ºæ¨¡å‹...")
        input_size = X_train.shape[2]  # ç‰¹å¾µç¶­åº¦
        num_classes = len(self.label_encoder.classes_)
        self.create_model(input_size, num_classes)
        
        # 6. è¨“ç·´æ¨¡å‹
        print("\næ­¥é©Ÿ 4: è¨“ç·´æ¨¡å‹...")
        train_losses, train_accuracies = self.train_model(
            X_train, y_train, epochs, batch_size, learning_rate
        )
        
        # 7. å„²å­˜æ¨¡å‹
        print("\næ­¥é©Ÿ 5: å„²å­˜æ¨¡å‹...")
        model_path = self.save_model(train_losses, train_accuracies)
        
        # 8. ç¹ªè£½è¨“ç·´æ›²ç·š
        print("\næ­¥é©Ÿ 6: ç¹ªè£½è¨“ç·´æ›²ç·š...")
        self.plot_training_curves(train_losses, train_accuracies)
        
        print("\n" + "=" * 70)
        print("æ¨¡å‹è¨“ç·´å®Œæˆ!")
        print(f"æœ€çµ‚è¨“ç·´æº–ç¢ºç‡: {train_accuracies[-1]:.2f}%")
        print(f"æ¨¡å‹æª”æ¡ˆ: {model_path}")
        print("=" * 70)

def main():
    """ä¸»å‡½æ•¸"""
    try:
        trainer = SignLanguageTrainer()
        trainer.run_training(
            epochs=50,          # è¨“ç·´é€±æœŸ
            batch_size=16,      # æ‰¹æ¬¡å¤§å°ï¼ˆè€ƒæ…®åˆ°GPUè¨˜æ†¶é«”é™åˆ¶ï¼‰
            learning_rate=0.001,# å­¸ç¿’ç‡
            sequence_length=20  # åºåˆ—é•·åº¦
        )
    except Exception as e:
        print(f"è¨“ç·´éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
