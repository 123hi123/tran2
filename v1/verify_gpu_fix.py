"""
é©—è­‰PyTorch CUDAä¿®å¾©çµæœ
"""
import torch

print("ğŸ” PyTorch CUDAä¿®å¾©é©—è­‰")
print("=" * 40)

try:
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
    print(f"CUDAå¯ç”¨æ€§: {'âœ… å¯ç”¨' if torch.cuda.is_available() else 'âŒ ä¸å¯ç”¨'}")
    
    if torch.cuda.is_available():
        print(f"GPUæ•¸é‡: {torch.cuda.device_count()}")
        print(f"GPUåç¨±: {torch.cuda.get_device_name(0)}")
        print(f"GPUè¨˜æ†¶é«”: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        
        # æ¸¬è©¦GPUæ“ä½œ
        print("\nğŸ§ª GPUåŠŸèƒ½æ¸¬è©¦:")
        device = torch.device('cuda')
        x = torch.randn(3, 3, device=device)
        y = torch.randn(3, 3, device=device)
        z = torch.mm(x, y)
        print(f"âœ… GPUçŸ©é™£é‹ç®—æˆåŠŸ: çµæœåœ¨ {z.device}")
        
        print("\nğŸ‰ æ­å–œï¼GPUå·²ç¶“å¯ä»¥æ­£å¸¸ä½¿ç”¨äº†ï¼")
        print("ç¾åœ¨å¯ä»¥é‹è¡Œè¨“ç·´ä»£ç¢¼ï¼Œæ‡‰è©²æœƒé¡¯ç¤º: ä½¿ç”¨è¨­å‚™: cuda:0")
        
    else:
        print("\nâŒ ä»ç„¶ç„¡æ³•ä½¿ç”¨GPU")
        print("å¯èƒ½éœ€è¦:")
        print("1. æª¢æŸ¥NVIDIAé©…å‹•")
        print("2. å˜—è©¦ä¸åŒçš„CUDAç‰ˆæœ¬")
        print("3. å®Œå…¨é‡æ–°å®‰è£ç’°å¢ƒ")
        
except Exception as e:
    print(f"âŒ æ¸¬è©¦å¤±æ•—: {e}")
    
print("=" * 40)
