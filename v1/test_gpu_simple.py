"""
ç°¡å–®çš„GPUæ¸¬è©¦è…³æœ¬ - å°ˆé–€æ¸¬è©¦è¨“ç·´ä»£ç¢¼çš„GPUä½¿ç”¨
Simple GPU test for training code
"""

import torch
import numpy as np

def test_gpu_basic():
    """åŸºæœ¬GPUæ¸¬è©¦"""
    print("=" * 50)
    print("ğŸ” åŸºæœ¬GPUæ¸¬è©¦")
    print("=" * 50)
    
    # 1. æª¢æŸ¥CUDAå¯ç”¨æ€§
    print(f"1. CUDAå¯ç”¨æ€§: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"2. GPUæ•¸é‡: {torch.cuda.device_count()}")
        print(f"3. ç•¶å‰GPU: {torch.cuda.get_device_name(0)}")
        print(f"4. CUDAç‰ˆæœ¬: {torch.version.cuda}")
        
        # å‰µå»ºè¨­å‚™å°è±¡ (æ¨¡æ“¬è¨“ç·´ä»£ç¢¼)
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"5. è¨­å‚™å°è±¡: {device}")
        
        return True
    else:
        print("âŒ CUDAä¸å¯ç”¨")
        return False

def test_model_gpu():
    """æ¸¬è©¦æ¨¡å‹GPUéƒ¨ç½²"""
    print("\nğŸ¤– æ¨¡å‹GPUæ¸¬è©¦")
    print("-" * 30)
    
    try:
        # å°å…¥GRUæ¨¡å‹ (å¦‚æœå¯èƒ½)
        try:
            from train_model_v1 import SignLanguageGRU
            print("âœ… æˆåŠŸå°å…¥SignLanguageGRU")
        except ImportError as e:
            print(f"âŒ ç„¡æ³•å°å…¥æ¨¡å‹: {e}")
            return False
        
        # å‰µå»ºè¨­å‚™
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ç›®æ¨™è¨­å‚™: {device}")
        
        # å‰µå»ºæ¨¡å‹
        model = SignLanguageGRU(
            input_size=163,  # ç‰¹å¾µç¶­åº¦
            hidden_size=128,
            num_layers=2,
            num_classes=5,   # å‡è¨­5å€‹é¡åˆ¥
            dropout=0.3
        )
        
        print(f"æ¨¡å‹å‰µå»ºæˆåŠŸï¼Œåƒæ•¸æ•¸é‡: {sum(p.numel() for p in model.parameters()):,}")
        
        # ç§»å‹•åˆ°GPU
        model = model.to(device)
        print(f"âœ… æ¨¡å‹å·²ç§»å‹•åˆ°: {next(model.parameters()).device}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹GPUæ¸¬è©¦å¤±æ•—: {e}")
        return False

def test_data_gpu():
    """æ¸¬è©¦è³‡æ–™GPUå‚³è¼¸"""
    print("\nğŸ“Š è³‡æ–™GPUå‚³è¼¸æ¸¬è©¦")
    print("-" * 30)
    
    try:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # å‰µå»ºæ¸¬è©¦è³‡æ–™ (æ¨¡æ“¬è¨“ç·´æ‰¹æ¬¡)
        batch_size = 4
        sequence_length = 20
        feature_dim = 163
        num_classes = 5
        
        # å‰µå»ºå‡è³‡æ–™
        X = torch.randn(batch_size, sequence_length, feature_dim)
        y = torch.randint(0, num_classes, (batch_size,))
        
        print(f"åŸå§‹è³‡æ–™è¨­å‚™: X={X.device}, y={y.device}")
        
        # ç§»å‹•åˆ°GPU (æ¨¡æ“¬è¨“ç·´ä»£ç¢¼)
        X_gpu = X.to(device)
        y_gpu = y.to(device)
        
        print(f"GPUè³‡æ–™è¨­å‚™: X={X_gpu.device}, y={y_gpu.device}")
        
        # æª¢æŸ¥æ˜¯å¦çœŸçš„åœ¨GPUä¸Š
        if torch.cuda.is_available():
            expected_device = f"cuda:{torch.cuda.current_device()}"
            if str(X_gpu.device) == expected_device and str(y_gpu.device) == expected_device:
                print("âœ… è³‡æ–™æˆåŠŸç§»å‹•åˆ°GPU")
                return True
            else:
                print(f"âŒ è³‡æ–™æœªæ­£ç¢ºç§»å‹•åˆ°GPU")
                return False
        else:
            print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œè³‡æ–™åœ¨CPUä¸Š")
            return False
            
    except Exception as e:
        print(f"âŒ è³‡æ–™GPUæ¸¬è©¦å¤±æ•—: {e}")
        return False

def test_training_simulation():
    """æ¨¡æ“¬è¨“ç·´éç¨‹"""
    print("\nğŸ‹ï¸ è¨“ç·´éç¨‹æ¨¡æ“¬")
    print("-" * 30)
    
    try:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ä½¿ç”¨è¨­å‚™: {device}")
        
        # æ¨¡æ“¬è¨“ç·´ä»£ç¢¼ä¸­çš„è¨­å‚™æª¢æŸ¥
        from train_model_v1 import SignLanguageTrainer
        trainer = SignLanguageTrainer()
        
        print(f"Trainerè¨­å‚™: {trainer.device}")
        
        if torch.cuda.is_available():
            # æª¢æŸ¥GPUè¨˜æ†¶é«”
            allocated = torch.cuda.memory_allocated()
            reserved = torch.cuda.memory_reserved()
            print(f"GPUè¨˜æ†¶é«” - å·²åˆ†é…: {allocated/1024**2:.1f}MB, å·²ä¿ç•™: {reserved/1024**2:.1f}MB")
            
            # ç°¡å–®çš„å‰å‘å‚³æ’­æ¸¬è©¦
            with torch.no_grad():
                test_input = torch.randn(2, 20, 163).to(device)
                print(f"æ¸¬è©¦è¼¸å…¥è¨­å‚™: {test_input.device}")
                
                # é€™è£¡æ‡‰è©²çœ‹åˆ°GPUè¨˜æ†¶é«”å¢åŠ 
                new_allocated = torch.cuda.memory_allocated()
                print(f"å‰µå»ºæ¸¬è©¦å¼µé‡å¾ŒGPUè¨˜æ†¶é«”: {new_allocated/1024**2:.1f}MB")
        
        return True
        
    except Exception as e:
        print(f"âŒ è¨“ç·´æ¨¡æ“¬å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False

def check_pytorch_installation_detailed():
    """è©³ç´°æª¢æŸ¥PyTorchå®‰è£"""
    print("\nğŸ”¥ PyTorchè©³ç´°æª¢æŸ¥")
    print("-" * 30)
    
    try:
        import torch
        print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
        print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"cuDNNç‰ˆæœ¬: {torch.backends.cudnn.version()}")
        print(f"æ˜¯å¦æ”¯æ´CUDA: {torch.cuda.is_available()}")
        
        if torch.cuda.is_available():
            print(f"CUDAè¨­å‚™æ•¸: {torch.cuda.device_count()}")
            print(f"ç•¶å‰è¨­å‚™: {torch.cuda.current_device()}")
            
            # æª¢æŸ¥è¨­å‚™å±¬æ€§
            props = torch.cuda.get_device_properties(0)
            print(f"è¨­å‚™åç¨±: {props.name}")
            print(f"è¨ˆç®—èƒ½åŠ›: {props.major}.{props.minor}")
            print(f"ç¸½è¨˜æ†¶é«”: {props.total_memory/1024**3:.1f}GB")
            
            return True
        else:
            print("âŒ CUDAä¸å¯ç”¨")
            
            # æä¾›å¯èƒ½çš„åŸå› 
            print("\nå¯èƒ½çš„åŸå› :")
            print("1. PyTorchæ²’æœ‰å®‰è£CUDAç‰ˆæœ¬")
            print("2. CUDAé©…å‹•ä¸ç›¸å®¹")
            print("3. ç’°å¢ƒè®Šæ•¸å•é¡Œ")
            
            return False
            
    except Exception as e:
        print(f"âŒ PyTorchæª¢æŸ¥å¤±æ•—: {e}")
        return False

def main():
    """ä¸»æ¸¬è©¦ç¨‹åº"""
    print("ğŸš€ GPUä½¿ç”¨è¨ºæ–·å·¥å…·")
    print("å°ˆé–€æª¢æŸ¥è¨“ç·´ä»£ç¢¼çš„GPUä½¿ç”¨å•é¡Œ")
    
    results = []
    
    # 1. åŸºæœ¬GPUæ¸¬è©¦
    results.append(("åŸºæœ¬GPU", test_gpu_basic()))
    
    # 2. PyTorchè©³ç´°æª¢æŸ¥
    results.append(("PyTorch", check_pytorch_installation_detailed()))
    
    # 3. æ¨¡å‹GPUæ¸¬è©¦
    results.append(("æ¨¡å‹GPU", test_model_gpu()))
    
    # 4. è³‡æ–™GPUæ¸¬è©¦
    results.append(("è³‡æ–™GPU", test_data_gpu()))
    
    # 5. è¨“ç·´æ¨¡æ“¬
    results.append(("è¨“ç·´æ¨¡æ“¬", test_training_simulation()))
    
    # ç¸½çµ
    print("\n" + "=" * 50)
    print("ğŸ“‹ æ¸¬è©¦çµæœç¸½çµ")
    print("=" * 50)
    
    for test_name, passed in results:
        status = "âœ… é€šé" if passed else "âŒ å¤±æ•—"
        print(f"{test_name:<10}: {status}")
    
    failed_tests = [name for name, passed in results if not passed]
    
    if not failed_tests:
        print("\nğŸ‰ æ‰€æœ‰æ¸¬è©¦é€šéï¼GPUæ‡‰è©²å¯ä»¥æ­£å¸¸ä½¿ç”¨ã€‚")
        print("\nå¦‚æœè¨“ç·´ä»£ç¢¼ä»ç„¶é¡¯ç¤ºä½¿ç”¨CPUï¼Œè«‹æª¢æŸ¥:")
        print("1. æ˜¯å¦åœ¨æ­£ç¢ºçš„condaç’°å¢ƒä¸­")
        print("2. PyTorchæ˜¯å¦ç‚ºCUDAç‰ˆæœ¬")
        print("3. é‡æ–°é‹è¡Œè¨“ç·´ä»£ç¢¼ä¸¦è§€å¯Ÿè¼¸å‡º")
    else:
        print(f"\nâš ï¸  å¤±æ•—çš„æ¸¬è©¦: {', '.join(failed_tests)}")
        print("\nğŸ’¡ å»ºè­°è§£æ±ºæ­¥é©Ÿ:")
        print("1. é‹è¡Œå®Œæ•´è¨ºæ–·: python v1/debug_gpu.py")
        print("2. é‡æ–°å®‰è£PyTorch CUDAç‰ˆæœ¬:")
        print("   conda uninstall pytorch torchvision torchaudio")
        print("   conda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia")
        print("3. ç¢ºä¿åœ¨sign_languageç’°å¢ƒä¸­é‹è¡Œ")

if __name__ == "__main__":
    main()
