"""
GPUä½¿ç”¨æƒ…æ³æª¢æ¸¬å’Œç›£æ§å·¥å…·
ç”¨æ–¼é©—è­‰å’Œç›£æ§æ‰‹èªè¾¨è­˜ç³»çµ±çš„GPUåŠ é€Ÿæƒ…æ³
"""

import torch
import numpy as np
import time
import psutil
import os

def check_gpu_availability():
    """æª¢æŸ¥GPUå¯ç”¨æ€§å’Œè©³ç´°è³‡è¨Š"""
    print("=" * 60)
    print("ğŸ” GPUç’°å¢ƒæª¢æŸ¥")
    print("=" * 60)
    
    # æª¢æŸ¥CUDAæ˜¯å¦å¯ç”¨
    cuda_available = torch.cuda.is_available()
    print(f"CUDAå¯ç”¨æ€§: {'âœ… å¯ç”¨' if cuda_available else 'âŒ ä¸å¯ç”¨'}")
    
    if cuda_available:
        # GPUæ•¸é‡
        gpu_count = torch.cuda.device_count()
        print(f"GPUæ•¸é‡: {gpu_count}")
        
        # ç•¶å‰GPUè³‡è¨Š
        current_device = torch.cuda.current_device()
        gpu_name = torch.cuda.get_device_name(current_device)
        print(f"ç•¶å‰GPU: {gpu_name}")
        
        # CUDAç‰ˆæœ¬
        cuda_version = torch.version.cuda
        print(f"CUDAç‰ˆæœ¬: {cuda_version}")
        
        # PyTorchç‰ˆæœ¬
        pytorch_version = torch.__version__
        print(f"PyTorchç‰ˆæœ¬: {pytorch_version}")
        
        # GPUè¨˜æ†¶é«”è³‡è¨Š
        total_memory = torch.cuda.get_device_properties(current_device).total_memory
        print(f"GPUç¸½è¨˜æ†¶é«”: {total_memory / 1024**3:.2f} GB")
        
        # ç•¶å‰è¨˜æ†¶é«”ä½¿ç”¨
        allocated_memory = torch.cuda.memory_allocated()
        reserved_memory = torch.cuda.memory_reserved()
        print(f"å·²åˆ†é…è¨˜æ†¶é«”: {allocated_memory / 1024**3:.2f} GB")
        print(f"å·²ä¿ç•™è¨˜æ†¶é«”: {reserved_memory / 1024**3:.2f} GB")
        print(f"å¯ç”¨è¨˜æ†¶é«”: {(total_memory - reserved_memory) / 1024**3:.2f} GB")
        
    else:
        print("âŒ GPUä¸å¯ç”¨ï¼Œå°‡ä½¿ç”¨CPUé€²è¡Œè¨ˆç®—")
        print("è«‹æª¢æŸ¥ï¼š")
        print("1. NVIDIAé©…å‹•æ˜¯å¦æ­£ç¢ºå®‰è£")
        print("2. CUDAå·¥å…·åŒ…æ˜¯å¦å®‰è£")
        print("3. PyTorchæ˜¯å¦å®‰è£CUDAç‰ˆæœ¬")
    
    print("=" * 60)
    return cuda_available

def benchmark_gpu_vs_cpu(batch_size=16, sequence_length=20, feature_dim=163):
    """æ¯”è¼ƒGPUå’ŒCPUçš„æ€§èƒ½å·®ç•°"""
    print("\nâš¡ GPU vs CPU æ€§èƒ½æ¸¬è©¦")
    print("-" * 40)
    
    # å‰µå»ºæ¸¬è©¦è³‡æ–™
    test_data = torch.randn(batch_size, sequence_length, feature_dim)
    
    # å‰µå»ºç°¡å–®çš„æ¸¬è©¦æ¨¡å‹
    from train_model_v1 import SignLanguageGRU
    model = SignLanguageGRU(
        input_size=feature_dim,
        hidden_size=128,
        num_layers=2,
        num_classes=10
    )
    
    # CPUæ¸¬è©¦
    print("æ¸¬è©¦CPUæ€§èƒ½...")
    model_cpu = model.to('cpu')
    data_cpu = test_data.to('cpu')
    
    start_time = time.time()
    for _ in range(10):  # é‹è¡Œ10æ¬¡å–å¹³å‡
        with torch.no_grad():
            _ = model_cpu(data_cpu)
    cpu_time = (time.time() - start_time) / 10
    print(f"CPUå¹³å‡æ¨ç†æ™‚é–“: {cpu_time:.4f}ç§’")
    
    # GPUæ¸¬è©¦ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if torch.cuda.is_available():
        print("æ¸¬è©¦GPUæ€§èƒ½...")
        model_gpu = model.to('cuda')
        data_gpu = test_data.to('cuda')
        
        # é ç†±GPU
        for _ in range(5):
            with torch.no_grad():
                _ = model_gpu(data_gpu)
        torch.cuda.synchronize()
        
        start_time = time.time()
        for _ in range(10):
            with torch.no_grad():
                _ = model_gpu(data_gpu)
        torch.cuda.synchronize()
        gpu_time = (time.time() - start_time) / 10
        
        print(f"GPUå¹³å‡æ¨ç†æ™‚é–“: {gpu_time:.4f}ç§’")
        speedup = cpu_time / gpu_time
        print(f"GPUåŠ é€Ÿå€æ•¸: {speedup:.2f}x")
        
        return speedup
    else:
        print("GPUä¸å¯ç”¨ï¼Œç„¡æ³•é€²è¡Œæ¯”è¼ƒ")
        return 1.0

def monitor_gpu_during_training():
    """è¨“ç·´éç¨‹ä¸­çš„GPUç›£æ§è£é£¾å™¨"""
    def decorator(train_func):
        def wrapper(*args, **kwargs):
            if torch.cuda.is_available():
                print("\nğŸ“Š é–‹å§‹GPUç›£æ§...")
                initial_memory = torch.cuda.memory_allocated()
                max_memory = initial_memory
                
                try:
                    result = train_func(*args, **kwargs)
                    
                    # è¨“ç·´å¾Œçš„è¨˜æ†¶é«”çµ±è¨ˆ
                    final_memory = torch.cuda.memory_allocated()
                    peak_memory = torch.cuda.max_memory_allocated()
                    
                    print("\nğŸ“ˆ GPUè¨˜æ†¶é«”ä½¿ç”¨çµ±è¨ˆ:")
                    print(f"åˆå§‹è¨˜æ†¶é«”: {initial_memory / 1024**3:.2f} GB")
                    print(f"æœ€çµ‚è¨˜æ†¶é«”: {final_memory / 1024**3:.2f} GB")
                    print(f"å³°å€¼è¨˜æ†¶é«”: {peak_memory / 1024**3:.2f} GB")
                    print(f"è¨˜æ†¶é«”å¢é•·: {(final_memory - initial_memory) / 1024**3:.2f} GB")
                    
                    return result
                    
                except Exception as e:
                    print(f"âŒ è¨“ç·´éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
                    if "out of memory" in str(e).lower():
                        print("ğŸ’¡ å»ºè­°ï¼šæ¸›å°‘batch_sizeæˆ–sequence_length")
                    raise e
            else:
                return train_func(*args, **kwargs)
        return wrapper
    return decorator

def estimate_memory_usage(batch_size, sequence_length, feature_dim, hidden_size=128, num_layers=2):
    """ä¼°ç®—æ¨¡å‹è¨˜æ†¶é«”ä½¿ç”¨é‡"""
    print(f"\nğŸ§® è¨˜æ†¶é«”ä½¿ç”¨ä¼°ç®— (batch_size={batch_size}, seq_len={sequence_length})")
    print("-" * 50)
    
    # æ¨¡å‹åƒæ•¸è¨˜æ†¶é«”
    # GRUåƒæ•¸æ•¸é‡ä¼°ç®—
    input_size = feature_dim
    gru_params = (input_size + hidden_size + 1) * hidden_size * 6 * num_layers  # é›™å‘GRU
    fc_params = hidden_size * 2 * hidden_size + hidden_size * 10  # å…¨é€£æ¥å±¤
    total_params = gru_params + fc_params
    
    model_memory = total_params * 4 / 1024**3  # 4 bytes per float32
    print(f"æ¨¡å‹åƒæ•¸è¨˜æ†¶é«”: {model_memory:.3f} GB")
    
    # è¼¸å…¥è³‡æ–™è¨˜æ†¶é«”
    input_memory = batch_size * sequence_length * feature_dim * 4 / 1024**3
    print(f"è¼¸å…¥è³‡æ–™è¨˜æ†¶é«”: {input_memory:.3f} GB")
    
    # ä¸­é–“æ¿€æ´»è¨˜æ†¶é«”ï¼ˆç²—ç•¥ä¼°ç®—ï¼‰
    activation_memory = batch_size * sequence_length * hidden_size * num_layers * 8 / 1024**3  # é›™å‘
    print(f"æ¿€æ´»è¨˜æ†¶é«”: {activation_memory:.3f} GB")
    
    # æ¢¯åº¦è¨˜æ†¶é«”ï¼ˆè¨“ç·´æ™‚ï¼‰
    gradient_memory = model_memory  # æ¢¯åº¦èˆ‡åƒæ•¸åŒå¤§å°
    print(f"æ¢¯åº¦è¨˜æ†¶é«”: {gradient_memory:.3f} GB")
    
    total_memory = model_memory + input_memory + activation_memory + gradient_memory
    print(f"ç¸½è¨ˆè¨˜æ†¶é«”: {total_memory:.3f} GB")
    
    # RTX A2000å»ºè­°
    if total_memory > 5.0:
        print("âš ï¸  è¨˜æ†¶é«”ä½¿ç”¨å¯èƒ½è¶…éRTX A2000é™åˆ¶(6GB)")
        print("ğŸ’¡ å»ºè­°èª¿æ•´åƒæ•¸:")
        suggested_batch = max(1, int(batch_size * 4.0 / total_memory))
        print(f"   - æ¸›å°‘batch_sizeåˆ°: {suggested_batch}")
        print(f"   - æˆ–æ¸›å°‘sequence_lengthåˆ°: {int(sequence_length * 0.8)}")
    else:
        print("âœ… è¨˜æ†¶é«”ä½¿ç”¨åœ¨RTX A2000å¯æ¥å—ç¯„åœå…§")
    
    return total_memory

def check_system_info():
    """æª¢æŸ¥ç³»çµ±ç¡¬é«”è³‡è¨Š"""
    print("\nğŸ’» ç³»çµ±ç¡¬é«”è³‡è¨Š")
    print("-" * 30)
    
    # CPUè³‡è¨Š
    print(f"CPUæ ¸å¿ƒæ•¸: {psutil.cpu_count(logical=False)}")
    print(f"CPUé‚è¼¯æ ¸å¿ƒ: {psutil.cpu_count(logical=True)}")
    
    # è¨˜æ†¶é«”è³‡è¨Š
    memory = psutil.virtual_memory()
    print(f"ç³»çµ±è¨˜æ†¶é«”: {memory.total / 1024**3:.1f} GB")
    print(f"å¯ç”¨è¨˜æ†¶é«”: {memory.available / 1024**3:.1f} GB")
    
    # æª¢æŸ¥æ˜¯å¦æœ‰GPU
    if torch.cuda.is_available():
        for i in range(torch.cuda.device_count()):
            props = torch.cuda.get_device_properties(i)
            print(f"GPU {i}: {props.name}")
            print(f"  è¨˜æ†¶é«”: {props.total_memory / 1024**3:.1f} GB")
            print(f"  è¨ˆç®—èƒ½åŠ›: {props.major}.{props.minor}")

def main():
    """ä¸»æª¢æ¸¬ç¨‹åº"""
    print("ğŸš€ æ‰‹èªè¾¨è­˜ç³»çµ±GPUæª¢æ¸¬å·¥å…·")
    
    # 1. æª¢æŸ¥ç³»çµ±è³‡è¨Š
    check_system_info()
    
    # 2. æª¢æŸ¥GPUå¯ç”¨æ€§
    gpu_available = check_gpu_availability()
    
    # 3. è¨˜æ†¶é«”ä½¿ç”¨ä¼°ç®—
    estimate_memory_usage(
        batch_size=16,
        sequence_length=20,
        feature_dim=163
    )
    
    # 4. æ€§èƒ½æ¸¬è©¦
    if gpu_available:
        speedup = benchmark_gpu_vs_cpu()
        
        print(f"\nâœ… GPUæª¢æ¸¬å®Œæˆ!")
        print(f"   - GPUåŠ é€Ÿ: {speedup:.1f}å€")
        print(f"   - å»ºè­°ä½¿ç”¨GPUé€²è¡Œè¨“ç·´")
    else:
        print(f"\nâš ï¸  GPUä¸å¯ç”¨ï¼Œå°‡ä½¿ç”¨CPU")
        print(f"   - è¨“ç·´é€Ÿåº¦æœƒè¼ƒæ…¢")
        print(f"   - å»ºè­°æª¢æŸ¥CUDAå®‰è£")

if __name__ == "__main__":
    main()
