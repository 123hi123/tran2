"""
GPUè¨ºæ–·å·¥å…· - æŸ¥æ‰¾GPUç„¡æ³•ä½¿ç”¨çš„åŸå› 
This script will help identify why GPU is not being used
"""

import sys
import os

def check_basic_python_info():
    """æª¢æŸ¥åŸºæœ¬Pythonç’°å¢ƒ"""
    print("ğŸ Pythonç’°å¢ƒæª¢æŸ¥")
    print("-" * 40)
    print(f"Pythonç‰ˆæœ¬: {sys.version}")
    print(f"PythonåŸ·è¡Œæª”è·¯å¾‘: {sys.executable}")
    print(f"ç•¶å‰å·¥ä½œç›®éŒ„: {os.getcwd()}")
    print()

def check_pytorch_installation():
    """æª¢æŸ¥PyTorchå®‰è£ç‹€æ³"""
    print("ğŸ”¥ PyTorchå®‰è£æª¢æŸ¥")
    print("-" * 40)
    
    try:
        import torch
        print(f"âœ… PyTorchå·²å®‰è£: ç‰ˆæœ¬ {torch.__version__}")
        
        # æª¢æŸ¥CUDAç‰ˆæœ¬
        cuda_version = torch.version.cuda
        print(f"PyTorch CUDAç‰ˆæœ¬: {cuda_version if cuda_version else 'âŒ ç„¡CUDAæ”¯æ´'}")
        
        # æª¢æŸ¥CUDAæ˜¯å¦å¯ç”¨
        cuda_available = torch.cuda.is_available()
        print(f"CUDAå¯ç”¨æ€§: {'âœ… å¯ç”¨' if cuda_available else 'âŒ ä¸å¯ç”¨'}")
        
        if cuda_available:
            print(f"CUDAè¨­å‚™æ•¸é‡: {torch.cuda.device_count()}")
            for i in range(torch.cuda.device_count()):
                print(f"  è¨­å‚™ {i}: {torch.cuda.get_device_name(i)}")
                props = torch.cuda.get_device_properties(i)
                print(f"    è¨˜æ†¶é«”: {props.total_memory / 1024**3:.1f} GB")
                print(f"    è¨ˆç®—èƒ½åŠ›: {props.major}.{props.minor}")
        else:
            print("âŒ ç„¡å¯ç”¨çš„CUDAè¨­å‚™")
            
        return cuda_available
        
    except ImportError as e:
        print(f"âŒ PyTorchæœªå®‰è£: {e}")
        return False
    except Exception as e:
        print(f"âŒ PyTorchæª¢æŸ¥å¤±æ•—: {e}")
        return False

def check_nvidia_driver():
    """æª¢æŸ¥NVIDIAé©…å‹•"""
    print("\nğŸ¯ NVIDIAé©…å‹•æª¢æŸ¥")
    print("-" * 40)
    
    try:
        import subprocess
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True, timeout=10)
        if result.returncode == 0:
            print("âœ… NVIDIAé©…å‹•æ­£å¸¸")
            print("nvidia-smiè¼¸å‡º:")
            print(result.stdout)
            return True
        else:
            print("âŒ nvidia-smiåŸ·è¡Œå¤±æ•—")
            print(f"éŒ¯èª¤: {result.stderr}")
            return False
    except subprocess.TimeoutExpired:
        print("âŒ nvidia-smiåŸ·è¡Œè¶…æ™‚")
        return False
    except FileNotFoundError:
        print("âŒ æ‰¾ä¸åˆ°nvidia-smiå‘½ä»¤")
        print("å¯èƒ½åŸå› : NVIDIAé©…å‹•æœªå®‰è£æˆ–æœªåŠ å…¥PATH")
        return False
    except Exception as e:
        print(f"âŒ æª¢æŸ¥NVIDIAé©…å‹•æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return False

def test_simple_gpu_operation():
    """æ¸¬è©¦ç°¡å–®çš„GPUæ“ä½œ"""
    print("\nâš¡ GPUæ“ä½œæ¸¬è©¦")
    print("-" * 40)
    
    try:
        import torch
        
        if not torch.cuda.is_available():
            print("âŒ CUDAä¸å¯ç”¨ï¼Œè·³éGPUæ¸¬è©¦")
            return False
            
        # æ¸¬è©¦åŸºæœ¬GPUæ“ä½œ
        print("æ¸¬è©¦1: å‰µå»ºGPUå¼µé‡")
        device = torch.device('cuda:0')
        x = torch.randn(3, 3, device=device)
        print(f"âœ… GPUå¼µé‡å‰µå»ºæˆåŠŸ: {x.device}")
        
        print("æ¸¬è©¦2: GPUçŸ©é™£é‹ç®—")
        y = torch.randn(3, 3, device=device)
        z = torch.mm(x, y)
        print(f"âœ… GPUé‹ç®—æˆåŠŸ: çµæœåœ¨ {z.device}")
        
        print("æ¸¬è©¦3: CPU-GPUè³‡æ–™å‚³è¼¸")
        cpu_tensor = torch.randn(3, 3)
        gpu_tensor = cpu_tensor.to(device)
        cpu_back = gpu_tensor.cpu()
        print("âœ… CPU-GPUè³‡æ–™å‚³è¼¸æˆåŠŸ")
        
        print("æ¸¬è©¦4: GPUè¨˜æ†¶é«”ç‹€æ…‹")
        allocated = torch.cuda.memory_allocated(device)
        reserved = torch.cuda.memory_reserved(device)
        print(f"âœ… å·²åˆ†é…è¨˜æ†¶é«”: {allocated / 1024**2:.1f} MB")
        print(f"âœ… å·²ä¿ç•™è¨˜æ†¶é«”: {reserved / 1024**2:.1f} MB")
        
        return True
        
    except Exception as e:
        print(f"âŒ GPUæ“ä½œæ¸¬è©¦å¤±æ•—: {e}")
        return False

def check_conda_environment():
    """æª¢æŸ¥Condaç’°å¢ƒ"""
    print("\nğŸ Condaç’°å¢ƒæª¢æŸ¥")
    print("-" * 40)
    
    # æª¢æŸ¥æ˜¯å¦åœ¨Condaç’°å¢ƒä¸­
    conda_env = os.environ.get('CONDA_DEFAULT_ENV')
    if conda_env:
        print(f"âœ… ç•¶å‰Condaç’°å¢ƒ: {conda_env}")
    else:
        print("âŒ ä¸åœ¨Condaç’°å¢ƒä¸­")
        return False
    
    # æª¢æŸ¥ç’°å¢ƒè·¯å¾‘
    conda_prefix = os.environ.get('CONDA_PREFIX')
    if conda_prefix:
        print(f"ç’°å¢ƒè·¯å¾‘: {conda_prefix}")
    
    # æª¢æŸ¥Pythonè·¯å¾‘æ˜¯å¦æŒ‡å‘Condaç’°å¢ƒ
    python_path = sys.executable
    if conda_prefix and conda_prefix in python_path:
        print("âœ… Pythonè·¯å¾‘æ­£ç¢ºæŒ‡å‘Condaç’°å¢ƒ")
        return True
    else:
        print("âŒ Pythonè·¯å¾‘å¯èƒ½ä¸æ­£ç¢º")
        print(f"æœŸæœ›åŒ…å«: {conda_prefix}")
        print(f"å¯¦éš›è·¯å¾‘: {python_path}")
        return False

def check_pytorch_cuda_installation():
    """è©³ç´°æª¢æŸ¥PyTorch CUDAå®‰è£"""
    print("\nğŸ” PyTorch CUDAè©³ç´°æª¢æŸ¥")
    print("-" * 40)
    
    try:
        import torch
        
        # æª¢æŸ¥ç·¨è­¯è³‡è¨Š
        print("PyTorchç·¨è­¯è³‡è¨Š:")
        print(f"  ç‰ˆæœ¬: {torch.__version__}")
        print(f"  CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"  cuDNNç‰ˆæœ¬: {torch.backends.cudnn.version()}")
        print(f"  æ˜¯å¦ä½¿ç”¨CUDAç·¨è­¯: {torch.version.cuda is not None}")
        
        # æª¢æŸ¥å¾Œç«¯
        print("\nCUDAå¾Œç«¯ç‹€æ…‹:")
        print(f"  cuDNNå¯ç”¨: {torch.backends.cudnn.enabled}")
        print(f"  cuDNNåŸºæº–æ¨¡å¼: {torch.backends.cudnn.benchmark}")
        
        # å˜—è©¦åˆ—å‡ºæ‰€æœ‰å¯ç”¨è¨­å‚™
        print("\nå¯ç”¨è¨­å‚™:")
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                device_name = torch.cuda.get_device_name(i)
                print(f"  cuda:{i} - {device_name}")
        else:
            print("  åªæœ‰CPUå¯ç”¨")
            
        return torch.cuda.is_available()
        
    except Exception as e:
        print(f"âŒ æª¢æŸ¥å¤±æ•—: {e}")
        return False

def provide_solutions():
    """æä¾›è§£æ±ºæ–¹æ¡ˆ"""
    print("\nğŸ’¡ å¸¸è¦‹å•é¡Œè§£æ±ºæ–¹æ¡ˆ")
    print("=" * 50)
    
    print("\nå•é¡Œ1: PyTorchæ²’æœ‰CUDAæ”¯æ´")
    print("è§£æ±ºæ–¹æ¡ˆ:")
    print("  conda uninstall pytorch torchvision torchaudio")
    print("  conda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia")
    
    print("\nå•é¡Œ2: NVIDIAé©…å‹•å•é¡Œ")
    print("è§£æ±ºæ–¹æ¡ˆ:")
    print("  1. æ›´æ–°NVIDIAé©…å‹•åˆ°æœ€æ–°ç‰ˆæœ¬")
    print("  2. é‡å•Ÿé›»è…¦")
    print("  3. æª¢æŸ¥è¨­å‚™ç®¡ç†å“¡ä¸­GPUç‹€æ…‹")
    
    print("\nå•é¡Œ3: CUDAç‰ˆæœ¬ä¸åŒ¹é…")
    print("è§£æ±ºæ–¹æ¡ˆ:")
    print("  1. æª¢æŸ¥ nvidia-smi é¡¯ç¤ºçš„CUDAç‰ˆæœ¬")
    print("  2. å®‰è£åŒ¹é…çš„PyTorchç‰ˆæœ¬")
    print("  3. è¨ªå• https://pytorch.org/ ç²å–æ­£ç¢ºå®‰è£å‘½ä»¤")
    
    print("\nå•é¡Œ4: ç’°å¢ƒè®Šæ•¸å•é¡Œ")
    print("è§£æ±ºæ–¹æ¡ˆ:")
    print("  1. ç¢ºä¿åœ¨æ­£ç¢ºçš„Condaç’°å¢ƒä¸­")
    print("  2. conda activate sign_language")
    print("  3. é‡æ–°å®‰è£PyTorch")
    
    print("\nå•é¡Œ5: æ¬Šé™å•é¡Œ")
    print("è§£æ±ºæ–¹æ¡ˆ:")
    print("  1. ä»¥ç®¡ç†å“¡èº«ä»½é‹è¡ŒPowerShell")
    print("  2. æª¢æŸ¥é˜²æ¯’è»Ÿé«”æ˜¯å¦é˜»æ“‹")

def main():
    """ä¸»è¨ºæ–·ç¨‹åº"""
    print("ğŸš¨ GPUè¨ºæ–·å·¥å…· - æŸ¥æ‰¾GPUç„¡æ³•ä½¿ç”¨çš„åŸå› ")
    print("=" * 60)
    
    # æ”¶é›†æ‰€æœ‰æª¢æŸ¥çµæœ
    results = {}
    
    # 1. åŸºæœ¬ç’°å¢ƒæª¢æŸ¥
    check_basic_python_info()
    
    # 2. Condaç’°å¢ƒæª¢æŸ¥
    results['conda'] = check_conda_environment()
    
    # 3. PyTorchå®‰è£æª¢æŸ¥
    results['pytorch'] = check_pytorch_installation()
    
    # 4. NVIDIAé©…å‹•æª¢æŸ¥
    results['nvidia'] = check_nvidia_driver()
    
    # 5. PyTorch CUDAè©³ç´°æª¢æŸ¥
    results['pytorch_cuda'] = check_pytorch_cuda_installation()
    
    # 6. GPUæ“ä½œæ¸¬è©¦
    results['gpu_ops'] = test_simple_gpu_operation()
    
    # ç¸½çµè¨ºæ–·çµæœ
    print("\nğŸ“‹ è¨ºæ–·çµæœç¸½çµ")
    print("=" * 60)
    
    all_passed = True
    for check_name, result in results.items():
        status = "âœ… é€šé" if result else "âŒ å¤±æ•—"
        print(f"{check_name:15}: {status}")
        if not result:
            all_passed = False
    
    if all_passed:
        print("\nğŸ‰ æ‰€æœ‰æª¢æŸ¥éƒ½é€šéï¼GPUæ‡‰è©²å¯ä»¥æ­£å¸¸ä½¿ç”¨ã€‚")
        print("å¦‚æœä»£ç¢¼ä»ç„¶ä½¿ç”¨CPUï¼Œè«‹æª¢æŸ¥ä»£ç¢¼ä¸­çš„è¨­å‚™è¨­å®šã€‚")
    else:
        print("\nâš ï¸  ç™¼ç¾å•é¡Œï¼Œè«‹æŸ¥çœ‹ä¸Šè¿°å¤±æ•—çš„æª¢æŸ¥é …ç›®ã€‚")
        provide_solutions()
    
    # å¿«é€Ÿæ¸¬è©¦ä»£ç¢¼
    print("\nğŸ§ª å¿«é€Ÿæ¸¬è©¦ä»£ç¢¼:")
    print("import torch")
    print("print(f'CUDAå¯ç”¨: {torch.cuda.is_available()}')")
    print("if torch.cuda.is_available():")
    print("    print(f'GPUåç¨±: {torch.cuda.get_device_name(0)}')")
    print("    x = torch.randn(3, 3, device='cuda')")
    print("    print(f'GPUå¼µé‡: {x.device}')")

if __name__ == "__main__":
    main()
