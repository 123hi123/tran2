"""
æ‰‹èªè¾¨è­˜è³‡æ–™é è™•ç†è…³æœ¬ v1
åŠŸèƒ½ï¼š
1. è®€å–æ‰€æœ‰ä»¥signé–‹é ­çš„CSVæ–‡ä»¶
2. æŒ‰sign_languageé¡åˆ¥åˆ†çµ„è™•ç†
3. åˆ†å‰²ç‚ºè¨“ç·´é›†å’Œæ¸¬è©¦é›†
4. å°ä¸è¶³5ç­†è³‡æ–™çš„é¡åˆ¥é€²è¡Œè³‡æ–™å¢å¼·
5. å„²å­˜è™•ç†å¾Œçš„è³‡æ–™é›†
"""

import pandas as pd
import numpy as np
import os
import glob
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import warnings
warnings.filterwarnings('ignore')

class SignLanguageDataProcessor:
    def __init__(self, dataset_folder="dataset", output_folder="v1/processed_data"):
        self.dataset_folder = dataset_folder
        self.output_folder = output_folder
        self.feature_columns = None
        self.label_encoder = LabelEncoder()
        
    def setup_directories(self):
        """å»ºç«‹è¼¸å‡ºè³‡æ–™å¤¾"""
        os.makedirs(self.output_folder, exist_ok=True)
        print(f"è¼¸å‡ºè³‡æ–™å¤¾å·²å»ºç«‹: {self.output_folder}")
    
    def define_feature_columns(self):
        """å®šç¾©ç‰¹å¾µæ¬„ä½ï¼ˆæ’é™¤source_videoï¼‰"""
        # å§¿æ…‹ç‰¹å¾µé» (11-22)
        pose_features = []
        for i in range(11, 23):
            pose_features.extend([f'pose_tag{i}_x', f'pose_tag{i}_y', f'pose_tag{i}_z'])
        
        # å·¦æ‰‹ç‰¹å¾µé» (0-20)
        left_hand_features = []
        for i in range(21):
            left_hand_features.extend([f'Left_hand_tag{i}_x', f'Left_hand_tag{i}_y', f'Left_hand_tag{i}_z'])
        
        # å³æ‰‹ç‰¹å¾µé» (0-20)
        right_hand_features = []
        for i in range(21):
            right_hand_features.extend([f'Right_hand_tag{i}_x', f'Right_hand_tag{i}_y', f'Right_hand_tag{i}_z'])
        
        # çµ„åˆæ‰€æœ‰æ¬„ä½ï¼ˆframeç”¨æ–¼æ’åºï¼Œpose/handåº§æ¨™ç”¨æ–¼ç‰¹å¾µï¼‰
        self.feature_columns = ['frame'] + pose_features + left_hand_features + right_hand_features
        print(f"ç¸½æ¬„ä½æ•¸é‡: {len(self.feature_columns)} (åŒ…å«frame)")
        print(f"å¯¦éš›ç‰¹å¾µç¶­åº¦: {len(pose_features + left_hand_features + right_hand_features)} (ä¸åŒ…å«frame)")
    
    def load_all_csv_files(self):
        """è¼‰å…¥æ‰€æœ‰ä»¥signé–‹é ­çš„CSVæ–‡ä»¶"""
        csv_pattern = os.path.join(self.dataset_folder, "sign*.csv")
        csv_files = glob.glob(csv_pattern)
        
        if not csv_files:
            raise ValueError(f"åœ¨ {self.dataset_folder} ä¸­æ‰¾ä¸åˆ°ä»¥signé–‹é ­çš„CSVæ–‡ä»¶")
        
        print(f"æ‰¾åˆ° {len(csv_files)} å€‹CSVæ–‡ä»¶")
        
        all_data = []
        for file_path in csv_files:
            try:
                df = pd.read_csv(file_path)
                print(f"è¼‰å…¥æ–‡ä»¶: {os.path.basename(file_path)} - å½¢ç‹€: {df.shape}")
                all_data.append(df)
            except Exception as e:
                print(f"è¼‰å…¥æ–‡ä»¶ {file_path} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        
        if not all_data:
            raise ValueError("æ²’æœ‰æˆåŠŸè¼‰å…¥ä»»ä½•CSVæ–‡ä»¶")
        
        # åˆä½µæ‰€æœ‰è³‡æ–™
        combined_data = pd.concat(all_data, ignore_index=True)
        print(f"åˆä½µå¾Œç¸½è³‡æ–™å½¢ç‹€: {combined_data.shape}")
        
        # ğŸ”§ é‡è¦ä¿®æ­£ï¼šæŒ‰ sign_language å’Œ frame æ’åºï¼Œç¢ºä¿æ™‚é–“åºåˆ—æ­£ç¢º
        if 'frame' in combined_data.columns:
            print("æŒ‰ sign_language å’Œ frame æ’åºè³‡æ–™...")
            combined_data = combined_data.sort_values(['sign_language', 'frame']).reset_index(drop=True)
            print("âœ… æ™‚é–“åºåˆ—æ’åºå®Œæˆ")
        else:
            print("âš ï¸  è­¦å‘Šï¼šæ²’æœ‰ frame æ¬„ä½ï¼Œç„¡æ³•ä¿è­‰æ™‚é–“åºåˆ—é †åº")
        
        return combined_data
    
    def data_augmentation(self, data, noise_factor=0.01):
        """è³‡æ–™å¢å¼·ï¼šæ·»åŠ è¼•å¾®çš„éš¨æ©Ÿé›œè¨Š"""
        augmented_data = data.copy()
        
        # å°ç‰¹å¾µæ¬„ä½æ·»åŠ è¼•å¾®é›œè¨Šï¼ˆä¸åŒ…å«frameå’Œéæ•¸å€¼æ¬„ä½ï¼‰
        numeric_features = [col for col in self.feature_columns if col != 'frame']
        
        for col in numeric_features:
            if col in augmented_data.columns:
                # æ·»åŠ é«˜æ–¯é›œè¨Š
                noise = np.random.normal(0, noise_factor, size=len(augmented_data))
                augmented_data[col] = augmented_data[col] + noise
        
        return augmented_data
    
    def ensure_minimum_samples(self, grouped_data, min_samples=5):
        """ç¢ºä¿æ¯å€‹é¡åˆ¥è‡³å°‘æœ‰min_samplesç­†è³‡æ–™"""
        processed_groups = {}
        
        for sign_language, group_data in grouped_data.items():
            current_count = len(group_data)
            print(f"é¡åˆ¥ '{sign_language}': {current_count} ç­†è³‡æ–™", end="")
            
            if current_count < min_samples:
                # éœ€è¦å¢å¼·è³‡æ–™
                needed_samples = min_samples - current_count
                print(f" -> éœ€è¦å¢å¼· {needed_samples} ç­†")
                
                # é‡è¤‡åŸå§‹è³‡æ–™ä¸¦æ·»åŠ é›œè¨Š
                augmented_samples = []
                for i in range(needed_samples):
                    # éš¨æ©Ÿé¸æ“‡ä¸€ç­†åŸå§‹è³‡æ–™é€²è¡Œå¢å¼·
                    sample_idx = np.random.randint(0, current_count)
                    original_sample = group_data.iloc[[sample_idx]]
                    augmented_sample = self.data_augmentation(original_sample)
                    augmented_samples.append(augmented_sample)
                
                # åˆä½µåŸå§‹è³‡æ–™å’Œå¢å¼·è³‡æ–™
                enhanced_data = pd.concat([group_data] + augmented_samples, ignore_index=True)
                processed_groups[sign_language] = enhanced_data
            else:
                print(" -> è³‡æ–™å……è¶³")
                processed_groups[sign_language] = group_data
        
        return processed_groups
    
    def split_train_test(self, processed_groups, test_size=0.2, random_state=42):
        """åˆ†å‰²è¨“ç·´é›†å’Œæ¸¬è©¦é›†"""
        train_data = []
        test_data = []
        
        for sign_language, group_data in processed_groups.items():
            if len(group_data) < 2:
                # è³‡æ–™å¤ªå°‘ï¼Œå…¨éƒ¨æ”¾å…¥è¨“ç·´é›†
                train_data.append(group_data)
                print(f"é¡åˆ¥ '{sign_language}': è³‡æ–™å¤ªå°‘ï¼Œå…¨éƒ¨ä½œç‚ºè¨“ç·´é›†")
            else:
                # åˆ†å‰²è³‡æ–™
                train_group, test_group = train_test_split(
                    group_data, 
                    test_size=test_size, 
                    random_state=random_state,
                    stratify=None  # ç”±æ–¼æ˜¯æŒ‰é¡åˆ¥åˆ†çµ„ï¼Œä¸éœ€è¦åˆ†å±¤
                )
                train_data.append(train_group)
                test_data.append(test_group)
                print(f"é¡åˆ¥ '{sign_language}': è¨“ç·´é›† {len(train_group)} ç­†, æ¸¬è©¦é›† {len(test_group)} ç­†")
        
        # åˆä½µæ‰€æœ‰é¡åˆ¥çš„è³‡æ–™
        final_train = pd.concat(train_data, ignore_index=True) if train_data else pd.DataFrame()
        final_test = pd.concat(test_data, ignore_index=True) if test_data else pd.DataFrame()
        
        return final_train, final_test
    
    def preprocess_features(self, data):
        """æ”¹é€²çš„ç‰¹å¾µé è™•ç†ï¼Œæ™ºèƒ½è™•ç†ç¼ºå¤±å€¼"""
        print("\nğŸ”§ é–‹å§‹ç‰¹å¾µé è™•ç†...")
        processed_data = data.copy()
        
        # æª¢æŸ¥æ˜¯å¦æœ‰ç¼ºå¤±å€¼
        total_missing = processed_data.isnull().sum().sum()
        if total_missing > 0:
            print(f"ç™¼ç¾ {total_missing} å€‹ç¼ºå¤±å€¼ï¼Œå•Ÿå‹•æ™ºèƒ½è™•ç†...")
            
            # ä½¿ç”¨æ”¹é€²çš„ç¼ºå¤±å€¼è™•ç†å™¨
            try:
                import sys
                import os
                sys.path.append(os.path.dirname(os.path.dirname(__file__)))  # æ·»åŠ çˆ¶ç›®éŒ„åˆ°è·¯å¾‘
                from improved_missing_handler import ImprovedMissingValueProcessor
                
                processor = ImprovedMissingValueProcessor()
                
                # åˆ†æç¼ºå¤±æ¨¡å¼
                analysis = processor.analyze_missing_patterns(processed_data)
                
                # è¨ˆç®—ä¸­æ€§ä½ç½®
                processor.calculate_neutral_positions(processed_data)
                
                # æ™ºèƒ½æ’å€¼
                processed_data = processor.smart_interpolation(processed_data)
                
                print("âœ… æ™ºèƒ½ç¼ºå¤±å€¼è™•ç†å®Œæˆ")
                
            except ImportError:
                print("âš ï¸  ç„¡æ³•è¼‰å…¥æ”¹é€²çš„è™•ç†å™¨ï¼Œä½¿ç”¨åŸºç¤æ–¹æ³•...")
                # å›é€€åˆ°åŸºç¤æ–¹æ³•
                for col in self.feature_columns:
                    if col in processed_data.columns:
                        processed_data[col] = processed_data[col].fillna(0)
        else:
            print("âœ… æ²’æœ‰ç™¼ç¾ç¼ºå¤±å€¼")
        
        # æœ€çµ‚æª¢æŸ¥
        final_missing = processed_data.isnull().sum().sum()
        print(f"æœ€çµ‚ç¼ºå¤±å€¼: {final_missing}")
        
        return processed_data
    
    def save_datasets(self, train_data, test_data):
        """å„²å­˜è™•ç†å¾Œçš„è³‡æ–™é›†"""
        train_path = os.path.join(self.output_folder, "train_dataset.csv")
        test_path = os.path.join(self.output_folder, "test_dataset.csv")
        
        train_data.to_csv(train_path, index=False)
        test_data.to_csv(test_path, index=False)
        
        print(f"\nè³‡æ–™é›†å·²å„²å­˜:")
        print(f"è¨“ç·´é›†: {train_path} - å½¢ç‹€: {train_data.shape}")
        print(f"æ¸¬è©¦é›†: {test_path} - å½¢ç‹€: {test_data.shape}")
        
        # å„²å­˜æ¨™ç±¤ç·¨ç¢¼å™¨
        import joblib
        encoder_path = os.path.join(self.output_folder, "label_encoder.pkl")
        joblib.dump(self.label_encoder, encoder_path)
        print(f"æ¨™ç±¤ç·¨ç¢¼å™¨å·²å„²å­˜: {encoder_path}")
    
    def run_preprocessing(self):
        """åŸ·è¡Œå®Œæ•´çš„è³‡æ–™é è™•ç†æµç¨‹"""
        print("=" * 60)
        print("æ‰‹èªè¾¨è­˜è³‡æ–™é è™•ç† v1")
        print("=" * 60)
        
        # 1. å»ºç«‹è¼¸å‡ºè³‡æ–™å¤¾
        self.setup_directories()
        
        # 2. å®šç¾©ç‰¹å¾µæ¬„ä½
        self.define_feature_columns()
        
        # 3. è¼‰å…¥æ‰€æœ‰CSVæ–‡ä»¶
        print("\næ­¥é©Ÿ 1: è¼‰å…¥è³‡æ–™...")
        combined_data = self.load_all_csv_files()
        
        # 4. æª¢æŸ¥å¿…è¦æ¬„ä½
        if 'sign_language' not in combined_data.columns:
            raise ValueError("æ‰¾ä¸åˆ° 'sign_language' æ¬„ä½")
        
        # 5. ç§»é™¤source_videoæ¬„ä½ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if 'source_video' in combined_data.columns:
            combined_data = combined_data.drop('source_video', axis=1)
            print("å·²ç§»é™¤ 'source_video' æ¬„ä½")
        
        # 6. æŒ‰sign_languageåˆ†çµ„
        print("\næ­¥é©Ÿ 2: æŒ‰é¡åˆ¥åˆ†çµ„...")
        grouped_data = dict(list(combined_data.groupby('sign_language')))
        print(f"æ‰¾åˆ° {len(grouped_data)} å€‹ä¸åŒçš„æ‰‹èªé¡åˆ¥")
        
        # 7. ç¢ºä¿æ¯å€‹é¡åˆ¥è‡³å°‘æœ‰5ç­†è³‡æ–™
        print("\næ­¥é©Ÿ 3: è³‡æ–™å¢å¼·...")
        processed_groups = self.ensure_minimum_samples(grouped_data)
        
        # 8. åˆ†å‰²è¨“ç·´é›†å’Œæ¸¬è©¦é›†
        print("\næ­¥é©Ÿ 4: åˆ†å‰²è³‡æ–™é›†...")
        train_data, test_data = self.split_train_test(processed_groups)
        
        # 9. ç·¨ç¢¼æ¨™ç±¤
        print("\næ­¥é©Ÿ 5: ç·¨ç¢¼æ¨™ç±¤...")
        all_labels = pd.concat([train_data['sign_language'], test_data['sign_language']])
        self.label_encoder.fit(all_labels)
        
        train_data['sign_language_encoded'] = self.label_encoder.transform(train_data['sign_language'])
        if len(test_data) > 0:
            test_data['sign_language_encoded'] = self.label_encoder.transform(test_data['sign_language'])
        
        # 10. é è™•ç†ç‰¹å¾µ
        print("\næ­¥é©Ÿ 6: é è™•ç†ç‰¹å¾µ...")
        train_data = self.preprocess_features(train_data)
        if len(test_data) > 0:
            test_data = self.preprocess_features(test_data)
        
        # 11. å„²å­˜è³‡æ–™é›†
        print("\næ­¥é©Ÿ 7: å„²å­˜è³‡æ–™é›†...")
        self.save_datasets(train_data, test_data)
        
        print("\n" + "=" * 60)
        print("è³‡æ–™é è™•ç†å®Œæˆ!")
        print(f"é¡åˆ¥æ•¸é‡: {len(self.label_encoder.classes_)}")
        print(f"é¡åˆ¥åˆ—è¡¨: {list(self.label_encoder.classes_)}")
        print("=" * 60)

def main():
    """ä¸»å‡½æ•¸"""
    try:
        processor = SignLanguageDataProcessor()
        processor.run_preprocessing()
    except Exception as e:
        print(f"è™•ç†éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
