"""
å¿«é€Ÿéš¨æ©Ÿæ¸¬è©¦ - åŸºæ–¼çœŸå¯¦æ¸¬è©¦è…³æœ¬é‚è¼¯
ç›´æ¥é‹è¡Œï¼Œéš¨æ©Ÿé¸æ“‡5å€‹å‹•ä½œæ¸¬è©¦æ¨¡å‹é æ¸¬èƒ½åŠ›
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import joblib
import os
import random
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# è¨­å®šéš¨æ©Ÿç¨®å­ï¼Œè®“çµæœå¯é‡ç¾ï¼ˆå¦‚æœæƒ³è¦å®Œå…¨éš¨æ©Ÿï¼Œå¯ä»¥è¨»è§£æ‰é€™è¡Œï¼‰
random.seed(42)
np.random.seed(42)
torch.manual_seed(42)

class SignLanguageGRU(nn.Module):
    """æ‰‹èªè¾¨è­˜GRUæ¨¡å‹"""
    
    def __init__(self, input_size, hidden_size=128, num_layers=2, num_classes=10, dropout=0.3):
        super(SignLanguageGRU, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        self.gru = nn.GRU(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0,
            bidirectional=True
        )
        
        self.fc1 = nn.Linear(hidden_size * 2, hidden_size)
        self.dropout = nn.Dropout(dropout)
        self.fc2 = nn.Linear(hidden_size, num_classes)
        self.relu = nn.ReLU()
        
    def forward(self, x):
        gru_out, _ = self.gru(x)
        last_output = gru_out[:, -1, :]
        
        out = self.fc1(last_output)
        out = self.relu(out)
        out = self.dropout(out)
        out = self.fc2(out)
        
        return out

def preprocess_test_data(data):
    """é è™•ç†æ¸¬è©¦æ•¸æ“šï¼Œèˆ‡æ¸¬è©¦è…³æœ¬ä¿æŒä¸€è‡´"""
    total_missing = data.isnull().sum().sum()
    if total_missing > 0:
        print(f"âš ï¸  æ¸¬è©¦æ•¸æ“šç™¼ç¾ {total_missing} å€‹ç¼ºå¤±å€¼ï¼Œé€²è¡Œè™•ç†...")
        
        # å˜—è©¦ä½¿ç”¨æ”¹é€²çš„è™•ç†å™¨
        try:
            import sys
            import os
            sys.path.append(os.path.dirname(os.path.dirname(__file__)))
            from improved_missing_handler import ImprovedMissingValueProcessor
            
            processor = ImprovedMissingValueProcessor()
            processor.calculate_neutral_positions(data)
            data_processed = processor.smart_interpolation(data)
            
            # å°‡è™•ç†çµæœæ›´æ–°å›åŸæ•¸æ“š
            data.update(data_processed)
            print("âœ… æ™ºèƒ½ç¼ºå¤±å€¼è™•ç†å®Œæˆ")
            
        except ImportError:
            print("âš ï¸  ä½¿ç”¨åŸºç¤ç¼ºå¤±å€¼è™•ç†...")
            # åŸºç¤è™•ç†ï¼šå¡«å…… 0
            data.fillna(0, inplace=True)
    else:
        print("âœ… æ¸¬è©¦æ•¸æ“šæ²’æœ‰ç¼ºå¤±å€¼")

def prepare_all_test_sequences(data, sequence_length=20):
    """æº–å‚™æ‰€æœ‰æ¸¬è©¦åºåˆ— - èˆ‡æ¸¬è©¦è…³æœ¬é‚è¼¯å®Œå…¨ç›¸åŒ"""
    # ç‰¹å¾µæ¬„ä½ï¼ˆæ’é™¤æ¨™ç±¤ç›¸é—œæ¬„ä½å’Œframeï¼Œèˆ‡è¨“ç·´æ™‚ä¿æŒä¸€è‡´ï¼‰
    feature_cols = [col for col in data.columns 
                   if col not in ['sign_language', 'sign_language_encoded', 'frame', 'source_video']]
    
    print(f"æ¸¬è©¦ç‰¹å¾µç¶­åº¦: {len(feature_cols)} (æ’é™¤: sign_language, sign_language_encoded, frame, source_video)")
    
    # æŒ‰é¡åˆ¥åˆ†çµ„å‰µå»ºåºåˆ—
    sequences = []
    labels = []
    class_names = []
    sequence_info = []
    
    # æŒ‰sign_languageåˆ†çµ„
    for sign_language in data['sign_language'].unique():
        class_data = data[data['sign_language'] == sign_language]
        
        # å¦‚æœè³‡æ–™é•·åº¦è¶…ésequence_lengthï¼Œå‰µå»ºæ»‘å‹•çª—å£åºåˆ—
        if len(class_data) >= sequence_length:
            for i in range(len(class_data) - sequence_length + 1):
                seq = class_data.iloc[i:i+sequence_length][feature_cols].values
                sequences.append(seq)
                labels.append(class_data.iloc[i]['sign_language_encoded'])
                class_names.append(sign_language)
                # æ·»åŠ åºåˆ—ä¾†æºä¿¡æ¯
                start_frame = i + 1
                end_frame = i + sequence_length
                sequence_info.append(f"{sign_language}_seq_{start_frame}-{end_frame}")
        else:
            # å¦‚æœè³‡æ–™ä¸è¶³ï¼Œé€²è¡Œå¡«å……
            seq_data = class_data[feature_cols].values
            if len(seq_data) < sequence_length:
                # é‡è¤‡æœ€å¾Œä¸€å¹€ä¾†å¡«å……
                padding_needed = sequence_length - len(seq_data)
                last_frame = seq_data[-1:] if len(seq_data) > 0 else np.zeros((1, len(feature_cols)))
                padding = np.repeat(last_frame, padding_needed, axis=0)
                seq_data = np.vstack([seq_data, padding])
            
            sequences.append(seq_data)
            labels.append(class_data.iloc[0]['sign_language_encoded'])
            class_names.append(sign_language)
            sequence_info.append(f"{sign_language}_padded")
    
    sequences = np.array(sequences, dtype=np.float32)
    labels = np.array(labels, dtype=np.int64)
    
    print(f"æ¸¬è©¦åºåˆ—å½¢ç‹€: {sequences.shape}")
    print(f"æ¸¬è©¦æ¨™ç±¤å½¢ç‹€: {labels.shape}")
    
    return sequences, labels, class_names, sequence_info

def load_model_and_data():
    """è¼‰å…¥æ¨¡å‹å’Œæ•¸æ“š"""
    data_folder = "processed_data"
    model_folder = "models"
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    print(f"ğŸ”§ ä½¿ç”¨è¨­å‚™: {device}")
    
    # æ‰¾æœ€æ–°æ¨¡å‹
    model_files = [f for f in os.listdir(model_folder) if f.endswith('.pth')]
    if not model_files:
        raise FileNotFoundError("æ‰¾ä¸åˆ°ä»»ä½•æ¨¡å‹æª”æ¡ˆ")
    
    model_path = os.path.join(model_folder, sorted(model_files)[-1])
    print(f"ğŸ“ è¼‰å…¥æ¨¡å‹: {model_path}")
    
    # è¼‰å…¥æ¨¡å‹
    checkpoint = torch.load(model_path, map_location=device)
    
    # è¼‰å…¥æ¨™ç±¤ç·¨ç¢¼å™¨
    encoder_path = os.path.join(data_folder, "label_encoder.pkl")
    label_encoder = joblib.load(encoder_path)
    
    # å»ºç«‹æ¨¡å‹
    model_config = checkpoint['model_config']
    model = SignLanguageGRU(
        input_size=model_config['input_size'],
        hidden_size=model_config['hidden_size'],
        num_layers=model_config['num_layers'],
        num_classes=model_config['num_classes']
    ).to(device)
    
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    # è¼‰å…¥æ¸¬è©¦æ•¸æ“š
    test_path = os.path.join(data_folder, "test_dataset.csv")
    if not os.path.exists(test_path):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ¸¬è©¦è³‡æ–™é›†: {test_path}")
    
    test_data = pd.read_csv(test_path)
    
    # é è™•ç†æ¸¬è©¦æ•¸æ“š
    preprocess_test_data(test_data)
    
    print(f"âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸï¼Œé¡åˆ¥: {list(label_encoder.classes_)}")
    print(f"ğŸ“Š æ¸¬è©¦æ•¸æ“š: {test_data.shape}")
    
    return model, label_encoder, test_data, device

def predict_sequence(model, sequence, device):
    """é æ¸¬åºåˆ—"""
    sequence_tensor = torch.FloatTensor(sequence).unsqueeze(0).to(device)
    
    with torch.no_grad():
        outputs = model(sequence_tensor)
        probabilities = torch.softmax(outputs, dim=1)
        predicted_class = torch.argmax(outputs, dim=1).item()
        confidence = probabilities[0, predicted_class].item()
    
    return predicted_class, confidence, probabilities[0].cpu().numpy()

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸ¯ å¿«é€Ÿéš¨æ©Ÿå‹•ä½œé æ¸¬æ¸¬è©¦")
    print("=" * 60)
    
    try:
        # è¼‰å…¥æ¨¡å‹å’Œæ•¸æ“š
        model, label_encoder, test_data, device = load_model_and_data()
        
        # æº–å‚™æ‰€æœ‰æ¸¬è©¦åºåˆ—ï¼ˆä½¿ç”¨èˆ‡æ¸¬è©¦è…³æœ¬ç›¸åŒçš„é‚è¼¯ï¼‰
        X_test, y_test, class_names, sequence_info = prepare_all_test_sequences(test_data)
        
        if len(X_test) == 0:
            print("âŒ æ²’æœ‰å¯ç”¨çš„æ¸¬è©¦åºåˆ—")
            return
        
        # é€²è¡Œ5æ¬¡éš¨æ©Ÿæ¸¬è©¦
        num_tests = 5
        correct_predictions = 0
        
        for i in range(num_tests):
            print(f"\nğŸ² ç¬¬ {i+1} æ¬¡éš¨æ©Ÿæ¸¬è©¦")
            print("-" * 40)
            
            # éš¨æ©Ÿé¸æ“‡ä¸€å€‹åºåˆ—
            random_idx = random.randint(0, len(X_test) - 1)
            sequence = X_test[random_idx]
            true_label = y_test[random_idx]
            true_class = label_encoder.classes_[true_label]
            seq_info = sequence_info[random_idx]
            
            print(f"ğŸ“ åºåˆ—ç·¨è™Ÿ: {random_idx}")
            print(f"ğŸ“ åºåˆ—ä¾†æº: {seq_info}")
            
            # é æ¸¬
            predicted_label, confidence, all_probs = predict_sequence(model, sequence, device)
            predicted_class = label_encoder.classes_[predicted_label]
            
            # çµæœ
            is_correct = predicted_label == true_label
            if is_correct:
                correct_predictions += 1
            
            # é¡¯ç¤ºçµæœ
            print(f"ğŸ¯ å¯¦éš›å‹•ä½œ: {true_class}")
            print(f"ğŸ¤– é æ¸¬å‹•ä½œ: {predicted_class}")
            print(f"ğŸ“Š é æ¸¬ä¿¡å¿ƒ: {confidence:.4f} ({confidence*100:.1f}%)")
            
            if is_correct:
                print("âœ… é æ¸¬æ­£ç¢º! ğŸ‰")
            else:
                print("âŒ é æ¸¬éŒ¯èª¤...")
                
                # é¡¯ç¤ºå‰3åé æ¸¬
                prob_pairs = list(zip(label_encoder.classes_, all_probs))
                prob_pairs.sort(key=lambda x: x[1], reverse=True)
                print("å‰3åé æ¸¬:")
                for j, (class_name, prob) in enumerate(prob_pairs[:3]):
                    print(f"  {j+1}. {class_name}: {prob:.3f} ({prob*100:.1f}%)")
        
        # ç¸½çµ
        accuracy = correct_predictions / num_tests
        print(f"\nğŸ† æ¸¬è©¦ç¸½çµ")
        print("=" * 40)
        print(f"ç¸½æ¸¬è©¦æ¬¡æ•¸: {num_tests}")
        print(f"æ­£ç¢ºé æ¸¬: {correct_predictions}")
        print(f"é æ¸¬æº–ç¢ºç‡: {accuracy:.3f} ({accuracy*100:.1f}%)")
        
        if accuracy >= 0.8:
            print("ğŸŒŸ é€™æ‰¹éš¨æ©Ÿæ¨£æœ¬è¡¨ç¾å„ªç§€!")
        elif accuracy >= 0.6:
            print("ğŸ‘ é€™æ‰¹éš¨æ©Ÿæ¨£æœ¬è¡¨ç¾è‰¯å¥½!")
        else:
            print("ğŸ“ˆ é€™æ‰¹éš¨æ©Ÿæ¨£æœ¬é‚„æœ‰æ”¹é€²ç©ºé–“")
            
    except Exception as e:
        print(f"âŒ æ¸¬è©¦éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
