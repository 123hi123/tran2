"""
æ‰‹èªè¾¨è­˜GRUæ¨¡å‹é›†åˆ
åŒ…å«ä¸‰å€‹è¤‡é›œåº¦ç´šåˆ¥çš„æ¨¡å‹ï¼ŒåŸºæ–¼æ•¸æ“šåˆ†æçµæœè¨­è¨ˆ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Tuple, Optional

class SimpleGRU(nn.Module):
    """
    ç°¡å–®GRUæ¨¡å‹ - å¿«é€Ÿé©—è­‰ç”¨
    
    è¨­è¨ˆé‚è¼¯ï¼š
    - å–®å±¤GRUï¼Œå¿«é€Ÿè¨“ç·´
    - ç›®æ¨™ï¼š>70%æº–ç¢ºç‡ï¼Œ1-2å°æ™‚è¨“ç·´
    - ç”¨æ–¼é©—è­‰æ•¸æ“šç®¡é“å’ŒåŸºæœ¬å¯è¡Œæ€§
    """
    
    def __init__(self, 
                 input_size: int = 162,  # åŸºæ–¼æ•¸æ“šåˆ†æï¼šèº«é«”36+å·¦æ‰‹63+å³æ‰‹63
                 hidden_size: int = 64,
                 num_classes: int = 34,  # åŸºæ–¼æ•¸æ“šåˆ†æï¼š34ç¨®æ‰‹èª
                 dropout_rate: float = 0.3):
        super(SimpleGRU, self).__init__()
        
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_classes = num_classes
        
        # GRUå±¤
        self.gru = nn.GRU(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=1,
            batch_first=True,
            dropout=0  # å–®å±¤ä¸éœ€è¦dropout
        )
        
        # æ­£å‰‡åŒ–
        self.dropout = nn.Dropout(dropout_rate)
        
        # åˆ†é¡å±¤
        self.classifier = nn.Linear(hidden_size, num_classes)
        
        # åˆå§‹åŒ–æ¬Šé‡
        self._init_weights()
    
    def _init_weights(self):
        """æ¬Šé‡åˆå§‹åŒ–"""
        for name, param in self.gru.named_parameters():
            if 'weight' in name:
                nn.init.xavier_uniform_(param)
            elif 'bias' in name:
                nn.init.constant_(param, 0)
        
        nn.init.xavier_uniform_(self.classifier.weight)
        nn.init.constant_(self.classifier.bias, 0)
    
    def forward(self, x: torch.Tensor, hidden: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        å‰å‘å‚³æ’­
        
        Args:
            x: è¼¸å…¥åºåˆ— (batch_size, sequence_length, input_size)
            hidden: éš±è—ç‹€æ…‹ (å¯é¸)
            
        Returns:
            output: åˆ†é¡çµæœ (batch_size, num_classes)
        """
        batch_size, seq_len, _ = x.shape
        
        # GRUè™•ç†
        gru_out, _ = self.gru(x, hidden)  # (batch, seq, hidden)
        
        # ä½¿ç”¨æœ€å¾Œä¸€å€‹æ™‚é–“æ­¥çš„è¼¸å‡º
        last_output = gru_out[:, -1, :]  # (batch, hidden)
        
        # æ­£å‰‡åŒ–
        last_output = self.dropout(last_output)
        
        # åˆ†é¡
        output = self.classifier(last_output)  # (batch, num_classes)
        
        return output
    
    def get_model_info(self) -> dict:
        """ç²å–æ¨¡å‹ä¿¡æ¯"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        return {
            'model_name': 'SimpleGRU',
            'input_size': self.input_size,
            'hidden_size': self.hidden_size, 
            'num_classes': self.num_classes,
            'total_parameters': total_params,
            'trainable_parameters': trainable_params,
            'model_size_mb': total_params * 4 / (1024 * 1024)  # å‡è¨­float32
        }

class AttentionGRU(nn.Module):
    """
    æ³¨æ„åŠ›GRUæ¨¡å‹ - ä¸­ç­‰è¤‡é›œåº¦
    
    è¨­è¨ˆé‚è¼¯ï¼š
    - å¤šå±¤GRU + æ³¨æ„åŠ›æ©Ÿåˆ¶
    - ç›®æ¨™ï¼š>85%æº–ç¢ºç‡ï¼Œ3-5å°æ™‚è¨“ç·´
    - é—œæ³¨æ‰‹èªå‹•ä½œçš„é—œéµå¹€
    """
    
    def __init__(self,
                 input_size: int = 162,
                 hidden_size: int = 128,
                 num_layers: int = 2,
                 num_classes: int = 34,
                 num_attention_heads: int = 8,
                 dropout_rate: float = 0.4):
        super(AttentionGRU, self).__init__()
        
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.num_classes = num_classes
        self.num_attention_heads = num_attention_heads
        
        # GRUå±¤
        self.gru = nn.GRU(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout_rate if num_layers > 1 else 0,
            bidirectional=False
        )
        
        # æ³¨æ„åŠ›æ©Ÿåˆ¶
        self.attention = nn.MultiheadAttention(
            embed_dim=hidden_size,
            num_heads=num_attention_heads,
            dropout=dropout_rate,
            batch_first=True
        )
        
        # æ­£å‰‡åŒ–
        self.dropout = nn.Dropout(dropout_rate)
        self.layer_norm = nn.LayerNorm(hidden_size)
        
        # åˆ†é¡å™¨
        self.classifier = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_size // 2, num_classes)
        )
        
        # åˆå§‹åŒ–æ¬Šé‡
        self._init_weights()
    
    def _init_weights(self):
        """æ¬Šé‡åˆå§‹åŒ–"""
        for name, param in self.gru.named_parameters():
            if 'weight' in name:
                nn.init.xavier_uniform_(param)
            elif 'bias' in name:
                nn.init.constant_(param, 0)
        
        for layer in self.classifier:
            if isinstance(layer, nn.Linear):
                nn.init.xavier_uniform_(layer.weight)
                nn.init.constant_(layer.bias, 0)
    
    def forward(self, x: torch.Tensor, hidden: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        å‰å‘å‚³æ’­
        
        Args:
            x: è¼¸å…¥åºåˆ— (batch_size, sequence_length, input_size)
            hidden: éš±è—ç‹€æ…‹ (å¯é¸)
            
        Returns:
            output: åˆ†é¡çµæœ (batch_size, num_classes)
        """
        batch_size, seq_len, _ = x.shape
        
        # GRUè™•ç†
        gru_out, _ = self.gru(x, hidden)  # (batch, seq, hidden)
        
        # å±¤æ¨™æº–åŒ–
        gru_out = self.layer_norm(gru_out)
        
        # è‡ªæ³¨æ„åŠ›æ©Ÿåˆ¶
        attn_out, attn_weights = self.attention(
            query=gru_out,
            key=gru_out, 
            value=gru_out
        )  # (batch, seq, hidden)
        
        # æ®˜å·®é€£æ¥
        combined = gru_out + attn_out
        combined = self.dropout(combined)
        
        # å…¨å±€å¹³å‡æ± åŒ–ï¼ˆè€ƒæ…®æ‰€æœ‰æ™‚é–“æ­¥ï¼‰
        pooled = torch.mean(combined, dim=1)  # (batch, hidden)
        
        # åˆ†é¡
        output = self.classifier(pooled)  # (batch, num_classes)
        
        return output
    
    def get_attention_weights(self, x: torch.Tensor) -> torch.Tensor:
        """ç²å–æ³¨æ„åŠ›æ¬Šé‡ç”¨æ–¼å¯è¦–åŒ–"""
        with torch.no_grad():
            gru_out, _ = self.gru(x)
            gru_out = self.layer_norm(gru_out)
            _, attn_weights = self.attention(gru_out, gru_out, gru_out)
            return attn_weights
    
    def get_model_info(self) -> dict:
        """ç²å–æ¨¡å‹ä¿¡æ¯"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        return {
            'model_name': 'AttentionGRU',
            'input_size': self.input_size,
            'hidden_size': self.hidden_size,
            'num_layers': self.num_layers,
            'num_attention_heads': self.num_attention_heads,
            'num_classes': self.num_classes,
            'total_parameters': total_params,
            'trainable_parameters': trainable_params,
            'model_size_mb': total_params * 4 / (1024 * 1024)
        }

class BiGRUWithSelfAttention(nn.Module):
    """
    é›™å‘GRU + è‡ªæ³¨æ„åŠ›æ¨¡å‹ - é«˜è¤‡é›œåº¦
    
    è¨­è¨ˆé‚è¼¯ï¼š
    - é›™å‘GRU + å¤šé ­è‡ªæ³¨æ„åŠ›
    - ç›®æ¨™ï¼š>90%æº–ç¢ºç‡ï¼Œ8-12å°æ™‚è¨“ç·´
    - æœ€ä½³æ€§èƒ½ï¼Œé©åˆæœ€çµ‚éƒ¨ç½²
    """
    
    def __init__(self,
                 input_size: int = 162,
                 hidden_size: int = 256,
                 num_layers: int = 3,
                 num_classes: int = 34,
                 num_attention_heads: int = 16,
                 dropout_rate: float = 0.5):
        super(BiGRUWithSelfAttention, self).__init__()
        
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.num_classes = num_classes
        self.num_attention_heads = num_attention_heads
        
        # é›™å‘GRU
        self.bigru = nn.GRU(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout_rate if num_layers > 1 else 0,
            bidirectional=True
        )
        
        # é›™å‘è¼¸å‡ºçš„å°ºå¯¸æ˜¯ hidden_size * 2
        bigru_output_size = hidden_size * 2
        
        # å¤šé ­è‡ªæ³¨æ„åŠ›
        self.self_attention = nn.MultiheadAttention(
            embed_dim=bigru_output_size,
            num_heads=num_attention_heads,
            dropout=dropout_rate,
            batch_first=True
        )
        
        # ä½ç½®ç·¨ç¢¼ï¼ˆå¯é¸ï¼‰
        self.positional_encoding = self._create_positional_encoding(max_len=50, d_model=bigru_output_size)
        
        # æ­£å‰‡åŒ–å±¤
        self.dropout = nn.Dropout(dropout_rate)
        self.layer_norm1 = nn.LayerNorm(bigru_output_size)
        self.layer_norm2 = nn.LayerNorm(bigru_output_size)
        
        # åˆ†é¡å™¨
        self.classifier = nn.Sequential(
            nn.Linear(bigru_output_size, 256),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(128, num_classes)
        )
        
        # åˆå§‹åŒ–æ¬Šé‡
        self._init_weights()
    
    def _create_positional_encoding(self, max_len: int, d_model: int) -> torch.Tensor:
        """å‰µå»ºä½ç½®ç·¨ç¢¼"""
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        return pe.unsqueeze(0)  # (1, max_len, d_model)
    
    def _init_weights(self):
        """æ¬Šé‡åˆå§‹åŒ–"""
        for name, param in self.bigru.named_parameters():
            if 'weight' in name:
                nn.init.xavier_uniform_(param)
            elif 'bias' in name:
                nn.init.constant_(param, 0)
        
        for layer in self.classifier:
            if isinstance(layer, nn.Linear):
                nn.init.xavier_uniform_(layer.weight)
                nn.init.constant_(layer.bias, 0)
    
    def forward(self, x: torch.Tensor, hidden: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        å‰å‘å‚³æ’­
        
        Args:
            x: è¼¸å…¥åºåˆ— (batch_size, sequence_length, input_size)
            hidden: éš±è—ç‹€æ…‹ (å¯é¸)
            
        Returns:
            output: åˆ†é¡çµæœ (batch_size, num_classes)
        """
        batch_size, seq_len, _ = x.shape
        
        # é›™å‘GRUè™•ç†
        bigru_out, _ = self.bigru(x, hidden)  # (batch, seq, hidden*2)
        
        # æ·»åŠ ä½ç½®ç·¨ç¢¼
        if seq_len <= self.positional_encoding.shape[1]:
            pos_encoding = self.positional_encoding[:, :seq_len, :].to(x.device)
            bigru_out = bigru_out + pos_encoding
        
        # ç¬¬ä¸€æ¬¡å±¤æ¨™æº–åŒ–
        bigru_out = self.layer_norm1(bigru_out)
        
        # è‡ªæ³¨æ„åŠ›æ©Ÿåˆ¶
        attn_out, attn_weights = self.self_attention(
            query=bigru_out,
            key=bigru_out,
            value=bigru_out
        )  # (batch, seq, hidden*2)
        
        # æ®˜å·®é€£æ¥ + å±¤æ¨™æº–åŒ–
        combined = self.layer_norm2(bigru_out + attn_out)
        combined = self.dropout(combined)
        
        # å¤šç¨®æ± åŒ–ç­–ç•¥çš„çµ„åˆ
        max_pooled = torch.max(combined, dim=1)[0]  # (batch, hidden*2)
        avg_pooled = torch.mean(combined, dim=1)    # (batch, hidden*2)
        
        # çµ„åˆä¸åŒçš„æ± åŒ–çµæœ
        final_representation = max_pooled + avg_pooled  # (batch, hidden*2)
        
        # åˆ†é¡
        output = self.classifier(final_representation)  # (batch, num_classes)
        
        return output
    
    def get_attention_weights(self, x: torch.Tensor) -> torch.Tensor:
        """ç²å–æ³¨æ„åŠ›æ¬Šé‡ç”¨æ–¼å¯è¦–åŒ–"""
        with torch.no_grad():
            bigru_out, _ = self.bigru(x)
            
            if x.shape[1] <= self.positional_encoding.shape[1]:
                pos_encoding = self.positional_encoding[:, :x.shape[1], :].to(x.device)
                bigru_out = bigru_out + pos_encoding
                
            bigru_out = self.layer_norm1(bigru_out)
            _, attn_weights = self.self_attention(bigru_out, bigru_out, bigru_out)
            return attn_weights
    
    def get_model_info(self) -> dict:
        """ç²å–æ¨¡å‹ä¿¡æ¯"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        return {
            'model_name': 'BiGRUWithSelfAttention',
            'input_size': self.input_size,
            'hidden_size': self.hidden_size,
            'num_layers': self.num_layers,
            'num_attention_heads': self.num_attention_heads,
            'num_classes': self.num_classes,
            'total_parameters': total_params,
            'trainable_parameters': trainable_params,
            'model_size_mb': total_params * 4 / (1024 * 1024)
        }

def create_model(model_type: str = 'simple', **kwargs) -> nn.Module:
    """
    æ¨¡å‹å·¥å» å‡½æ•¸
    
    Args:
        model_type: æ¨¡å‹é¡å‹ ('simple', 'attention', 'bigru')
        **kwargs: æ¨¡å‹åƒæ•¸
        
    Returns:
        æ¨¡å‹å¯¦ä¾‹
    """
    model_classes = {
        'simple': SimpleGRU,
        'attention': AttentionGRU, 
        'bigru': BiGRUWithSelfAttention
    }
    
    if model_type not in model_classes:
        raise ValueError(f"Unknown model type: {model_type}. Available: {list(model_classes.keys())}")
    
    return model_classes[model_type](**kwargs)

def test_models():
    """æ¸¬è©¦æ‰€æœ‰æ¨¡å‹"""
    print("ğŸ§ª æ¸¬è©¦GRUæ¨¡å‹")
    print("=" * 50)
    
    # æ¨¡æ“¬è¼¸å…¥æ•¸æ“š
    batch_size = 8
    sequence_length = 30
    input_size = 162
    
    x = torch.randn(batch_size, sequence_length, input_size)
    print(f"ğŸ“Š è¼¸å…¥å½¢ç‹€: {x.shape}")
    
    models = {
        'simple': SimpleGRU(),
        'attention': AttentionGRU(),
        'bigru': BiGRUWithSelfAttention()
    }
    
    for name, model in models.items():
        print(f"\nğŸ” æ¸¬è©¦ {name.upper()} æ¨¡å‹:")
        
        # å‰å‘å‚³æ’­æ¸¬è©¦
        try:
            output = model(x)
            print(f"   âœ… è¼¸å‡ºå½¢ç‹€: {output.shape}")
            
            # æ¨¡å‹ä¿¡æ¯
            info = model.get_model_info()
            print(f"   ğŸ“ˆ åƒæ•¸æ•¸é‡: {info['total_parameters']:,}")
            print(f"   ğŸ’¾ æ¨¡å‹å¤§å°: {info['model_size_mb']:.2f} MB")
            
            # å…§å­˜ä½¿ç”¨ä¼°ç®—
            memory_mb = batch_size * sequence_length * input_size * 4 / (1024 * 1024)
            print(f"   ğŸ§  è¼¸å…¥è¨˜æ†¶é«”: {memory_mb:.2f} MB")
            
        except Exception as e:
            print(f"   âŒ æ¸¬è©¦å¤±æ•—: {str(e)}")

if __name__ == "__main__":
    test_models()
