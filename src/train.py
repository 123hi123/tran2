"""
æ‰‹èªè¾¨è­˜GRUæ¨¡å‹è¨“ç·´è…³æœ¬
åŸºæ–¼æ•¸æ“šåˆ†æçµæœçš„å„ªåŒ–è¨“ç·´ç­–ç•¥
"""

import os
import sys
import time
import json
from datetime import datetime
from typing import Dict, List, Tuple, Optional

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset, random_split
import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import seaborn as sns

# æ·»åŠ é …ç›®è·¯å¾‘
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.models.gru_models import create_model
from src.data_preprocessing import SignLanguagePreprocessor

class SignLanguageDataset(Dataset):
    """æ‰‹èªæ•¸æ“šé›†"""
    
    def __init__(self, sequences: np.ndarray, labels: np.ndarray, label_encoder: LabelEncoder):
        self.sequences = torch.FloatTensor(sequences)
        self.labels = torch.LongTensor(label_encoder.transform(labels))
        self.label_encoder = label_encoder
        
    def __len__(self):
        return len(self.sequences)
    
    def __getitem__(self, idx):
        return self.sequences[idx], self.labels[idx]

class EarlyStopping:
    """æ—©åœæ©Ÿåˆ¶"""
    
    def __init__(self, patience: int = 10, min_delta: float = 0.001, restore_best_weights: bool = True):
        self.patience = patience
        self.min_delta = min_delta
        self.restore_best_weights = restore_best_weights
        self.best_loss = float('inf')
        self.counter = 0
        self.best_weights = None
        
    def __call__(self, val_loss: float, model: nn.Module) -> bool:
        if val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
            if self.restore_best_weights:
                self.best_weights = model.state_dict().copy()
            return False
        else:
            self.counter += 1
            if self.counter >= self.patience:
                if self.restore_best_weights and self.best_weights:
                    model.load_state_dict(self.best_weights)
                return True
            return False

class TrainingConfig:
    """è¨“ç·´é…ç½®"""
    
    def __init__(self):
        # åŸºæ–¼æ•¸æ“šåˆ†æçš„é…ç½®
        self.batch_size = 16  # åŸºæ–¼è¨˜æ†¶é«”åˆ†æ
        self.learning_rate = 0.001
        self.num_epochs = 100
        self.early_stopping_patience = 15
        self.sequence_length = 30
        self.stride = 15
        
        # æ•¸æ“šåˆ†å‰²
        self.train_ratio = 0.7
        self.val_ratio = 0.15
        self.test_ratio = 0.15
        
        # è¨­å‚™é…ç½®
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # æ¨¡å‹ä¿å­˜
        self.save_dir = 'models'
        self.log_dir = 'logs'
        
        # å‰µå»ºç›®éŒ„
        os.makedirs(self.save_dir, exist_ok=True)
        os.makedirs(self.log_dir, exist_ok=True)

class SignLanguageTrainer:
    """æ‰‹èªè¾¨è­˜æ¨¡å‹è¨“ç·´å™¨"""
    
    def __init__(self, config: TrainingConfig):
        self.config = config
        self.device = config.device
        self.label_encoder = LabelEncoder()
        
        # è¨“ç·´æ­·å²
        self.train_history = {
            'train_loss': [],
            'train_acc': [],
            'val_loss': [],
            'val_acc': []
        }
        
        print(f"ğŸ”§ åˆå§‹åŒ–è¨“ç·´å™¨")
        print(f"   è¨­å‚™: {self.device}")
        print(f"   æ‰¹æ¬¡å¤§å°: {config.batch_size}")
        print(f"   å­¸ç¿’ç‡: {config.learning_rate}")
    
    def prepare_data(self, csv_files: List[str]) -> Tuple[DataLoader, DataLoader, DataLoader]:
        """
        æº–å‚™è¨“ç·´æ•¸æ“š
        
        Args:
            csv_files: CSVæª”æ¡ˆåˆ—è¡¨
            
        Returns:
            train_loader, val_loader, test_loader
        """
        print("ğŸ“Š æº–å‚™è¨“ç·´æ•¸æ“š...")
        
        # åˆå§‹åŒ–é è™•ç†å™¨
        preprocessor = SignLanguagePreprocessor(
            sequence_length=self.config.sequence_length,
            stride=self.config.stride
        )
        
        all_sequences = []
        all_labels = []
        
        # è™•ç†æ¯å€‹æª”æ¡ˆ
        for i, csv_file in enumerate(csv_files):
            print(f"   è™•ç†æª”æ¡ˆ {i+1}/{len(csv_files)}: {csv_file}")
            
            try:
                # è¼‰å…¥æ•¸æ“šï¼ˆå¯èƒ½éœ€è¦åˆ†æ‰¹è™•ç†å¤§æª”æ¡ˆï¼‰
                if os.path.getsize(csv_file) > 500 * 1024 * 1024:  # å¤§æ–¼500MB
                    # åˆ†æ‰¹è™•ç†
                    sequences, labels = self._process_large_file(csv_file, preprocessor)
                else:
                    df = pd.read_csv(csv_file)
                    df_clean = preprocessor.handle_missing_values(df)
                    df_normalized = preprocessor.normalize_coordinates(df_clean)
                    sequences, labels = preprocessor.create_sequences(df_normalized)
                
                if len(sequences) > 0:
                    all_sequences.append(sequences)
                    all_labels.extend(labels)
                    print(f"     ç”Ÿæˆåºåˆ—: {len(sequences)}")
                
            except Exception as e:
                print(f"     âŒ è™•ç†å¤±æ•—: {str(e)}")
                continue
        
        if not all_sequences:
            raise ValueError("æœªèƒ½ç”Ÿæˆä»»ä½•æœ‰æ•ˆåºåˆ—")
        
        # åˆä½µæ‰€æœ‰åºåˆ—
        final_sequences = np.concatenate(all_sequences, axis=0)
        final_labels = np.array(all_labels)
        
        print(f"ğŸ“ˆ ç¸½åºåˆ—æ•¸: {len(final_sequences)}")
        print(f"ğŸ·ï¸ æ¨™ç±¤ç¨®é¡: {len(np.unique(final_labels))}")
        
        # ç·¨ç¢¼æ¨™ç±¤
        self.label_encoder.fit(final_labels)
        num_classes = len(self.label_encoder.classes_)
        print(f"ğŸ“‹ é¡åˆ¥åˆ—è¡¨: {list(self.label_encoder.classes_)}")
        
        # å‰µå»ºæ•¸æ“šé›†
        dataset = SignLanguageDataset(final_sequences, final_labels, self.label_encoder)
        
        # åˆ†å‰²æ•¸æ“šé›†
        train_size = int(len(dataset) * self.config.train_ratio)
        val_size = int(len(dataset) * self.config.val_ratio)
        test_size = len(dataset) - train_size - val_size
        
        train_dataset, val_dataset, test_dataset = random_split(
            dataset, [train_size, val_size, test_size],
            generator=torch.Generator().manual_seed(42)
        )
        
        print(f"ğŸ“Š æ•¸æ“šåˆ†å‰²:")
        print(f"   è¨“ç·´é›†: {len(train_dataset)}")
        print(f"   é©—è­‰é›†: {len(val_dataset)}")
        print(f"   æ¸¬è©¦é›†: {len(test_dataset)}")
        
        # å‰µå»ºæ•¸æ“šè¼‰å…¥å™¨
        train_loader = DataLoader(
            train_dataset, 
            batch_size=self.config.batch_size,
            shuffle=True,
            num_workers=0  # Windowså…¼å®¹æ€§
        )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=self.config.batch_size,
            shuffle=False,
            num_workers=0
        )
        
        test_loader = DataLoader(
            test_dataset,
            batch_size=self.config.batch_size,
            shuffle=False,
            num_workers=0
        )
        
        return train_loader, val_loader, test_loader, num_classes
    
    def _process_large_file(self, csv_file: str, preprocessor: SignLanguagePreprocessor) -> Tuple[np.ndarray, List]:
        """è™•ç†å¤§æª”æ¡ˆ"""
        print(f"     ğŸ“¦ åˆ†æ‰¹è™•ç†å¤§æª”æ¡ˆ...")
        
        all_sequences = []
        all_labels = []
        
        chunk_size = 10000
        for chunk in pd.read_csv(csv_file, chunksize=chunk_size):
            try:
                chunk_clean = preprocessor.handle_missing_values(chunk)
                chunk_normalized = preprocessor.normalize_coordinates(chunk_clean)
                sequences, labels = preprocessor.create_sequences(chunk_normalized)
                
                if len(sequences) > 0:
                    all_sequences.append(sequences)
                    all_labels.extend(labels)
                    
            except Exception as e:
                print(f"       âš ï¸ å¡Šè™•ç†è­¦å‘Š: {str(e)}")
                continue
        
        if all_sequences:
            return np.concatenate(all_sequences, axis=0), all_labels
        else:
            return np.array([]), []
    
    def create_model(self, model_type: str, num_classes: int, input_size: int = 162) -> nn.Module:
        """å‰µå»ºæ¨¡å‹"""
        model = create_model(
            model_type=model_type,
            input_size=input_size,
            num_classes=num_classes
        )
        
        model = model.to(self.device)
        
        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        info = model.get_model_info()
        print(f"ğŸ¤– å‰µå»ºæ¨¡å‹: {info['model_name']}")
        print(f"   åƒæ•¸æ•¸é‡: {info['total_parameters']:,}")
        print(f"   æ¨¡å‹å¤§å°: {info['model_size_mb']:.2f} MB")
        
        return model
    
    def calculate_class_weights(self, train_loader: DataLoader) -> torch.Tensor:
        """è¨ˆç®—é¡åˆ¥æ¬Šé‡è™•ç†ä¸å¹³è¡¡å•é¡Œ"""
        print("âš–ï¸ è¨ˆç®—é¡åˆ¥æ¬Šé‡...")
        
        # çµ±è¨ˆå„é¡åˆ¥æ¨£æœ¬æ•¸
        class_counts = torch.zeros(len(self.label_encoder.classes_))
        
        for _, labels in train_loader:
            for label in labels:
                class_counts[label] += 1
        
        # è¨ˆç®—æ¬Šé‡ï¼ˆåæ¯”ä¾‹ï¼‰
        total_samples = class_counts.sum()
        class_weights = total_samples / (len(class_counts) * class_counts)
        
        # é¿å…ç„¡çª®å¤§
        class_weights = torch.where(class_counts == 0, torch.tensor(1.0), class_weights)
        
        print(f"   é¡åˆ¥åˆ†å¸ƒ: {class_counts.numpy()}")
        print(f"   é¡åˆ¥æ¬Šé‡: {class_weights.numpy()}")
        
        return class_weights.to(self.device)
    
    def train_model(self, model: nn.Module, train_loader: DataLoader, 
                   val_loader: DataLoader, model_name: str = "model") -> Dict:
        """è¨“ç·´æ¨¡å‹"""
        print(f"ğŸš€ é–‹å§‹è¨“ç·´æ¨¡å‹: {model_name}")
        print("=" * 60)
        
        # è¨ˆç®—é¡åˆ¥æ¬Šé‡
        class_weights = self.calculate_class_weights(train_loader)
        
        # æå¤±å‡½æ•¸å’Œå„ªåŒ–å™¨
        criterion = nn.CrossEntropyLoss(weight=class_weights)
        optimizer = optim.Adam(model.parameters(), lr=self.config.learning_rate)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', patience=5, factor=0.5, verbose=True
        )
        
        # æ—©åœæ©Ÿåˆ¶
        early_stopping = EarlyStopping(patience=self.config.early_stopping_patience)
        
        # è¨“ç·´å¾ªç’°
        start_time = time.time()
        best_val_acc = 0.0
        
        for epoch in range(self.config.num_epochs):
            # è¨“ç·´éšæ®µ
            train_loss, train_acc = self._train_epoch(model, train_loader, criterion, optimizer)
            
            # é©—è­‰éšæ®µ
            val_loss, val_acc = self._validate_epoch(model, val_loader, criterion)
            
            # å­¸ç¿’ç‡èª¿åº¦
            scheduler.step(val_loss)
            
            # è¨˜éŒ„æ­·å²
            self.train_history['train_loss'].append(train_loss)
            self.train_history['train_acc'].append(train_acc)
            self.train_history['val_loss'].append(val_loss)
            self.train_history['val_acc'].append(val_acc)
            
            # æ‰“å°é€²åº¦
            elapsed_time = time.time() - start_time
            print(f"Epoch {epoch+1:3d}/{self.config.num_epochs} | "
                  f"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.3f} | "
                  f"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.3f} | "
                  f"Time: {elapsed_time:.1f}s")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                self._save_model(model, model_name, epoch, val_acc)
            
            # æ—©åœæª¢æŸ¥
            if early_stopping(val_loss, model):
                print(f"ğŸ›‘ æ—©åœæ–¼ epoch {epoch+1}")
                break
        
        total_time = time.time() - start_time
        print(f"âœ… è¨“ç·´å®Œæˆï¼Œç¸½æ™‚é–“: {total_time:.1f}ç§’")
        print(f"ğŸ† æœ€ä½³é©—è­‰æº–ç¢ºç‡: {best_val_acc:.3f}")
        
        return {
            'best_val_acc': best_val_acc,
            'total_epochs': epoch + 1,
            'total_time': total_time,
            'final_lr': optimizer.param_groups[0]['lr']
        }
    
    def _train_epoch(self, model: nn.Module, train_loader: DataLoader, 
                    criterion: nn.Module, optimizer: optim.Optimizer) -> Tuple[float, float]:
        """è¨“ç·´ä¸€å€‹epoch"""
        model.train()
        total_loss = 0.0
        correct = 0
        total = 0
        
        for batch_idx, (sequences, labels) in enumerate(train_loader):
            sequences, labels = sequences.to(self.device), labels.to(self.device)
            
            # å‰å‘å‚³æ’­
            optimizer.zero_grad()
            outputs = model(sequences)
            loss = criterion(outputs, labels)
            
            # åå‘å‚³æ’­
            loss.backward()
            
            # æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            # çµ±è¨ˆ
            total_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
        
        avg_loss = total_loss / len(train_loader)
        accuracy = correct / total
        
        return avg_loss, accuracy
    
    def _validate_epoch(self, model: nn.Module, val_loader: DataLoader, 
                       criterion: nn.Module) -> Tuple[float, float]:
        """é©—è­‰ä¸€å€‹epoch"""
        model.eval()
        total_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for sequences, labels in val_loader:
                sequences, labels = sequences.to(self.device), labels.to(self.device)
                
                outputs = model(sequences)
                loss = criterion(outputs, labels)
                
                total_loss += loss.item()
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()
        
        avg_loss = total_loss / len(val_loader)
        accuracy = correct / total
        
        return avg_loss, accuracy
    
    def _save_model(self, model: nn.Module, model_name: str, epoch: int, val_acc: float):
        """ä¿å­˜æ¨¡å‹"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{model_name}_epoch{epoch:03d}_acc{val_acc:.3f}_{timestamp}.pth"
        filepath = os.path.join(self.config.save_dir, filename)
        
        torch.save({
            'model_state_dict': model.state_dict(),
            'epoch': epoch,
            'val_acc': val_acc,
            'label_encoder': self.label_encoder,
            'config': self.config.__dict__
        }, filepath)
        
        print(f"ğŸ’¾ ä¿å­˜æ¨¡å‹: {filename}")
    
    def evaluate_model(self, model: nn.Module, test_loader: DataLoader) -> Dict:
        """è©•ä¼°æ¨¡å‹"""
        print("ğŸ“Š æ¨¡å‹è©•ä¼°...")
        
        model.eval()
        all_predictions = []
        all_labels = []
        
        with torch.no_grad():
            for sequences, labels in test_loader:
                sequences = sequences.to(self.device)
                outputs = model(sequences)
                _, predicted = outputs.max(1)
                
                all_predictions.extend(predicted.cpu().numpy())
                all_labels.extend(labels.numpy())
        
        # è¨ˆç®—æŒ‡æ¨™
        accuracy = accuracy_score(all_labels, all_predictions)
        f1 = f1_score(all_labels, all_predictions, average='weighted')
        
        # æ··æ·†çŸ©é™£
        cm = confusion_matrix(all_labels, all_predictions)
        
        # åˆ†é¡å ±å‘Š
        class_names = self.label_encoder.classes_
        report = classification_report(
            all_labels, all_predictions,
            target_names=class_names,
            output_dict=True
        )
        
        print(f"ğŸ¯ æ¸¬è©¦æº–ç¢ºç‡: {accuracy:.3f}")
        print(f"ğŸ“ˆ F1åˆ†æ•¸: {f1:.3f}")
        
        return {
            'accuracy': accuracy,
            'f1_score': f1,
            'confusion_matrix': cm,
            'classification_report': report,
            'predictions': all_predictions,
            'true_labels': all_labels
        }
    
    def plot_training_history(self, save_path: Optional[str] = None):
        """ç¹ªè£½è¨“ç·´æ­·å²"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
        
        # æå¤±æ›²ç·š
        ax1.plot(self.train_history['train_loss'], label='è¨“ç·´æå¤±')
        ax1.plot(self.train_history['val_loss'], label='é©—è­‰æå¤±')
        ax1.set_title('æå¤±æ›²ç·š')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.legend()
        ax1.grid(True)
        
        # æº–ç¢ºç‡æ›²ç·š
        ax2.plot(self.train_history['train_acc'], label='è¨“ç·´æº–ç¢ºç‡')
        ax2.plot(self.train_history['val_acc'], label='é©—è­‰æº–ç¢ºç‡')
        ax2.set_title('æº–ç¢ºç‡æ›²ç·š')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Accuracy')
        ax2.legend()
        ax2.grid(True)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š è¨“ç·´æ›²ç·šä¿å­˜è‡³: {save_path}")
        
        plt.show()

def main():
    """ä¸»è¨“ç·´æµç¨‹"""
    print("ğŸš€ æ‰‹èªè¾¨è­˜GRUæ¨¡å‹è¨“ç·´")
    print("ğŸ• é–‹å§‹æ™‚é–“:", datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("=" * 60)
    
    # é…ç½®
    config = TrainingConfig()
    trainer = SignLanguageTrainer(config)
    
    # æ•¸æ“šæª”æ¡ˆï¼ˆæ ¹æ“šå¯¦éš›è·¯å¾‘èª¿æ•´ï¼‰
    csv_files = [
        "dataset/sign_language1.csv",
        "dataset/sign_language2.csv",
        # å¯ä»¥æ·»åŠ æ›´å¤šæª”æ¡ˆï¼Œä½†å»ºè­°å…ˆç”¨å°æ¨£æœ¬æ¸¬è©¦
    ]
    
    # æª¢æŸ¥æª”æ¡ˆå­˜åœ¨æ€§
    available_files = [f for f in csv_files if os.path.exists(f)]
    if not available_files:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•æ•¸æ“šæª”æ¡ˆ")
        return
    
    print(f"ğŸ“„ æ‰¾åˆ° {len(available_files)} å€‹æ•¸æ“šæª”æ¡ˆ")
    
    try:
        # æº–å‚™æ•¸æ“š
        train_loader, val_loader, test_loader, num_classes = trainer.prepare_data(available_files)
        
        # è¨“ç·´ä¸åŒè¤‡é›œåº¦çš„æ¨¡å‹
        models_to_train = [
            ('simple', 'ç°¡å–®GRUæ¨¡å‹'),
            ('attention', 'æ³¨æ„åŠ›GRUæ¨¡å‹'),
            # ('bigru', 'é›™å‘GRUæ¨¡å‹')  # å¯é¸ï¼Œæ™‚é–“è¼ƒé•·
        ]
        
        results = {}
        
        for model_type, model_desc in models_to_train:
            print(f"\nğŸ¤– è¨“ç·´ {model_desc}")
            print("-" * 40)
            
            # å‰µå»ºæ¨¡å‹
            model = trainer.create_model(model_type, num_classes)
            
            # è¨“ç·´
            train_result = trainer.train_model(model, train_loader, val_loader, model_type)
            
            # è©•ä¼°
            eval_result = trainer.evaluate_model(model, test_loader)
            
            # ä¿å­˜çµæœ
            results[model_type] = {
                'train': train_result,
                'eval': eval_result
            }
            
            # ç¹ªè£½è¨“ç·´æ›²ç·š
            trainer.plot_training_history(
                save_path=f"{config.log_dir}/{model_type}_training_history.png"
            )
            
            # é‡ç½®è¨“ç·´æ­·å²
            trainer.train_history = {
                'train_loss': [], 'train_acc': [],
                'val_loss': [], 'val_acc': []
            }
        
        # ç¸½çµçµæœ
        print("\nğŸ“Š è¨“ç·´ç¸½çµ")
        print("=" * 60)
        
        for model_type, result in results.items():
            print(f"ğŸ¤– {model_type.upper()} æ¨¡å‹:")
            print(f"   æœ€ä½³é©—è­‰æº–ç¢ºç‡: {result['train']['best_val_acc']:.3f}")
            print(f"   æ¸¬è©¦æº–ç¢ºç‡: {result['eval']['accuracy']:.3f}")
            print(f"   F1åˆ†æ•¸: {result['eval']['f1_score']:.3f}")
            print(f"   è¨“ç·´æ™‚é–“: {result['train']['total_time']:.1f}ç§’")
            print()
        
        # ä¿å­˜å®Œæ•´çµæœ
        with open(f"{config.log_dir}/training_results.json", 'w', encoding='utf-8') as f:
            # åºåˆ—åŒ–çµæœï¼ˆç§»é™¤ä¸å¯åºåˆ—åŒ–çš„å°è±¡ï¼‰
            serializable_results = {}
            for model_type, result in results.items():
                serializable_results[model_type] = {
                    'train': result['train'],
                    'eval': {
                        'accuracy': result['eval']['accuracy'],
                        'f1_score': result['eval']['f1_score']
                    }
                }
            json.dump(serializable_results, f, indent=2, ensure_ascii=False)
        
        print("âœ… è¨“ç·´æµç¨‹å®Œæˆ!")
        print("ğŸ• çµæŸæ™‚é–“:", datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
        
    except Exception as e:
        print(f"âŒ è¨“ç·´éç¨‹å‡ºéŒ¯: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
