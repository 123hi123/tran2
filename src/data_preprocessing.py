"""
æ‰‹èªè¾¨è­˜æ•¸æ“šé è™•ç†æ¨¡çµ„
åŸºæ–¼æ•¸æ“šåˆ†æçµæœï¼š121è¬æ¨£æœ¬ï¼Œ34é¡ï¼Œå³æ‰‹66%ç¼ºå¤±ï¼Œå·¦æ‰‹10%ç¼ºå¤±

ä¸»è¦åŠŸèƒ½ï¼š
1. ç¼ºå¤±å€¼æ™ºèƒ½æ’å€¼
2. åº§æ¨™æ¨™æº–åŒ–
3. åºåˆ—åˆ‡å‰²èˆ‡ç”Ÿæˆ
4. æ•¸æ“šå¢å¼·
"""

import pandas as pd
import numpy as np
from typing import List, Tuple, Dict
import warnings
warnings.filterwarnings('ignore')

class SignLanguagePreprocessor:
    def __init__(self, sequence_length: int = 30, stride: int = 15):
        """
        åƒæ•¸è¨­å®šåŸºæ–¼æ•¸æ“šåˆ†æï¼š
        - sequence_length=30: ç´„1ç§’æ‰‹èªå‹•ä½œï¼Œå¹³è¡¡ä¿¡æ¯å®Œæ•´æ€§èˆ‡è¨ˆç®—æ•ˆç‡
        - stride=15: 50%é‡ç–Šï¼Œå¾121è¬å¹€ç”Ÿæˆç´„8è¬åºåˆ—
        """
        self.sequence_length = sequence_length
        self.stride = stride
        
        # ç‰¹å¾µæ¬„ä½å®šç¾©ï¼ˆåŸºæ–¼CSVåˆ†æï¼‰
        self.pose_columns = [f'pose_tag{i}_{axis}' for i in range(11, 23) for axis in ['x', 'y', 'z']]
        self.left_hand_columns = [f'Left_hand_tag{i}_{axis}' for i in range(21) for axis in ['x', 'y', 'z']]
        self.right_hand_columns = [f'Right_hand_tag{i}_{axis}' for i in range(21) for axis in ['x', 'y', 'z']]
        
        # çµ±è¨ˆä¿¡æ¯ï¼ˆç”¨æ–¼æ¨™æº–åŒ–ï¼‰
        self.pose_stats = None
        self.hand_stats = None
        
    def load_and_analyze_data(self, csv_files: List[str]) -> Dict:
        """
        è¼‰å…¥å’Œåˆ†ææ•¸æ“š
        è¿”å›çµ±è¨ˆä¿¡æ¯ç”¨æ–¼å¾ŒçºŒè™•ç†
        """
        print("ğŸ” è¼‰å…¥æ•¸æ“šä¸¦åˆ†æçµ±è¨ˆç‰¹æ€§...")
        
        all_stats = {
            'total_samples': 0,
            'sign_language_counts': {},
            'missing_rates': {},
            'coordinate_stats': {}
        }
        
        for file_path in csv_files:
            print(f"ğŸ“„ è™•ç†æª”æ¡ˆ: {file_path}")
            
            # åˆ†å¡Šè®€å–å¤§æª”æ¡ˆ
            chunk_stats = []
            for chunk in pd.read_csv(file_path, chunksize=10000):
                chunk_stats.append(self._analyze_chunk(chunk))
                
            # åˆä½µçµ±è¨ˆä¿¡æ¯
            file_stats = self._merge_chunk_stats(chunk_stats)
            all_stats = self._merge_file_stats(all_stats, file_stats)
            
        return all_stats
    
    def handle_missing_values(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æ™ºèƒ½ç¼ºå¤±å€¼è™•ç†ç­–ç•¥
        
        åŸºæ–¼åˆ†æçµæœï¼š
        - å³æ‰‹ç¼ºå¤±66%ï¼šå¯èƒ½æ˜¯å·¦æ‰‹ç‚ºä¸»çš„æ‰‹èª
        - å·¦æ‰‹ç¼ºå¤±10%ï¼šå¶çˆ¾çš„æª¢æ¸¬å¤±æ•—
        
        ç­–ç•¥ï¼š
        1. æ™‚åºç·šæ€§æ’å€¼ï¼ˆå‰å¾Œå¹€ï¼‰
        2. åŸºæ–¼èº«é«”å§¿æ…‹çš„ç´„æŸæ’å€¼  
        3. æ¥µç«¯æƒ…æ³ä½¿ç”¨èº«é«”ä¸­å¿ƒé»
        """
        df_processed = df.copy()
        
        print("ğŸ”§ è™•ç†ç¼ºå¤±å€¼...")
        
        # 1. æ™‚åºæ’å€¼ï¼ˆçµ„å…§æ’å€¼ï¼ŒæŒ‰è¦–é »åˆ†çµ„ï¼‰
        for video in df['source_video'].unique():
            video_mask = df['source_video'] == video
            video_data = df_processed[video_mask].copy()
            
            # æŒ‰å¹€é †åºæ’åº
            video_data = video_data.sort_values('frame')
            
            # å°åº§æ¨™æ¬„ä½é€²è¡Œæ’å€¼
            coordinate_columns = self.pose_columns + self.left_hand_columns + self.right_hand_columns
            for col in coordinate_columns:
                if col in video_data.columns:
                    # ç·šæ€§æ’å€¼
                    video_data[col] = video_data[col].interpolate(method='linear', limit_direction='both')
                    
                    # å¦‚æœé–‹é ­æˆ–çµå°¾ä»æœ‰ç¼ºå¤±ï¼Œä½¿ç”¨å‰å‘/å¾Œå‘å¡«å……
                    video_data[col] = video_data[col].fillna(method='bfill').fillna(method='ffill')
            
            df_processed.loc[video_mask] = video_data
        
        # 2. åŸºæ–¼èº«é«”å§¿æ…‹çš„ç´„æŸæ’å€¼
        df_processed = self._pose_constrained_interpolation(df_processed)
        
        # 3. æœ€å¾Œçš„ç¼ºå¤±å€¼è™•ç†ï¼ˆä½¿ç”¨èº«é«”ä¸­å¿ƒé»ï¼‰
        df_processed = self._fill_remaining_missing(df_processed)
        
        return df_processed
    
    def normalize_coordinates(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        åº§æ¨™æ¨™æº–åŒ–è™•ç†
        
        ç›®æ¨™ï¼š
        1. æ¶ˆé™¤å€‹äººå·®ç•°ï¼ˆèº«é«˜ã€é«”å‹ï¼‰
        2. æ¶ˆé™¤ä½ç½®åå·®ï¼ˆç«™ç«‹ä½ç½®ï¼‰
        3. çµ±ä¸€åº§æ¨™å°ºåº¦
        """
        df_normalized = df.copy()
        
        print("ğŸ“ æ¨™æº–åŒ–åº§æ¨™...")
        
        # è¨ˆç®—èº«é«”ä¸­å¿ƒé»ï¼ˆè‚©è†€ä¸­é»ï¼‰
        if 'pose_tag11_x' in df.columns and 'pose_tag12_x' in df.columns:
            body_center_x = (df['pose_tag11_x'] + df['pose_tag12_x']) / 2
            body_center_y = (df['pose_tag11_y'] + df['pose_tag12_y']) / 2
            
            # è¨ˆç®—èº«é«”å¤§å°ï¼ˆè‚©å¯¬ä½œç‚ºåƒè€ƒï¼‰
            body_scale = np.sqrt((df['pose_tag11_x'] - df['pose_tag12_x'])**2 + 
                               (df['pose_tag11_y'] - df['pose_tag12_y'])**2)
            body_scale = body_scale.replace(0, 1)  # é¿å…é™¤é›¶
            
            # æ¨™æº–åŒ–æ‰€æœ‰åº§æ¨™
            coordinate_columns = self.pose_columns + self.left_hand_columns + self.right_hand_columns
            
            for col in coordinate_columns:
                if col in df.columns:
                    if col.endswith('_x'):
                        # Xåº§æ¨™ä»¥èº«é«”ä¸­å¿ƒç‚ºåŸé»ï¼ŒæŒ‰èº«é«”å¤§å°ç¸®æ”¾
                        df_normalized[col] = (df[col] - body_center_x) / body_scale
                    elif col.endswith('_y'):
                        # Yåº§æ¨™ä»¥èº«é«”ä¸­å¿ƒç‚ºåŸé»ï¼ŒæŒ‰èº«é«”å¤§å°ç¸®æ”¾
                        df_normalized[col] = (df[col] - body_center_y) / body_scale
                    elif col.endswith('_z'):
                        # Zåº§æ¨™æŒ‰èº«é«”å¤§å°ç¸®æ”¾
                        df_normalized[col] = df[col] / body_scale
        
        return df_normalized
    
    def create_sequences(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """
        å‰µå»ºè¨“ç·´åºåˆ—
        
        ç­–ç•¥ï¼š
        1. æ»‘å‹•çª—å£ç”Ÿæˆåºåˆ—
        2. å›ºå®šé•·åº¦ï¼ˆ30å¹€ï¼‰
        3. æ¨™ç±¤å°æ‡‰
        
        è¿”å›ï¼š
        - sequences: (N, sequence_length, feature_dim)
        - labels: (N,)
        """
        print("ğŸ”„ ç”Ÿæˆè¨“ç·´åºåˆ—...")
        
        sequences = []
        labels = []
        
        # æŒ‰è¦–é »åˆ†çµ„è™•ç†
        for video in df['source_video'].unique():
            video_data = df[df['source_video'] == video].copy()
            video_data = video_data.sort_values('frame')
            
            # æå–ç‰¹å¾µå’Œæ¨™ç±¤
            feature_columns = self.pose_columns + self.left_hand_columns + self.right_hand_columns
            available_features = [col for col in feature_columns if col in video_data.columns]
            
            features = video_data[available_features].values
            sign_language = video_data['sign_language'].iloc[0]  # å‡è¨­åŒä¸€è¦–é »åŒä¸€æ¨™ç±¤
            
            # æ»‘å‹•çª—å£ç”Ÿæˆåºåˆ—
            for start_idx in range(0, len(features) - self.sequence_length + 1, self.stride):
                end_idx = start_idx + self.sequence_length
                sequence = features[start_idx:end_idx]
                
                # æª¢æŸ¥åºåˆ—å®Œæ•´æ€§
                if not np.isnan(sequence).all():  # ä¸æ˜¯å…¨NaN
                    sequences.append(sequence)
                    labels.append(sign_language)
        
        sequences = np.array(sequences)
        labels = np.array(labels)
        
        print(f"âœ… ç”Ÿæˆ {len(sequences)} å€‹åºåˆ—")
        print(f"ğŸ“Š åºåˆ—å½¢ç‹€: {sequences.shape}")
        print(f"ğŸ·ï¸ æ¨™ç±¤ç¨®é¡: {len(np.unique(labels))}")
        
        return sequences, labels
    
    def augment_sequences(self, sequences: np.ndarray, labels: np.ndarray, 
                         augment_factor: float = 2.0) -> Tuple[np.ndarray, np.ndarray]:
        """
        æ•¸æ“šå¢å¼·
        
        ç­–ç•¥ï¼š
        1. æ™‚é–“æ‰­æ›²ï¼ˆæ™‚é–“è»¸æ‹‰ä¼¸/å£“ç¸®ï¼‰
        2. ç©ºé–“è®Šæ›ï¼ˆæ—‹è½‰ã€ç¸®æ”¾ï¼‰
        3. å™ªè²æ·»åŠ 
        """
        print("ğŸ­ æ•¸æ“šå¢å¼·...")
        
        augmented_sequences = [sequences]
        augmented_labels = [labels]
        
        n_augmentations = int(augment_factor - 1)
        
        for _ in range(n_augmentations):
            # æ™‚é–“æ‰­æ›²
            time_warped = self._time_warp(sequences)
            augmented_sequences.append(time_warped)
            augmented_labels.append(labels)
            
            # ç©ºé–“è®Šæ›
            spatially_transformed = self._spatial_transform(sequences)
            augmented_sequences.append(spatially_transformed)
            augmented_labels.append(labels)
            
            # å™ªè²æ·»åŠ 
            noisy = self._add_noise(sequences)
            augmented_sequences.append(noisy)
            augmented_labels.append(labels)
        
        final_sequences = np.concatenate(augmented_sequences, axis=0)
        final_labels = np.concatenate(augmented_labels, axis=0)
        
        print(f"âœ… å¢å¼·å¾Œåºåˆ—æ•¸é‡: {len(final_sequences)}")
        
        return final_sequences, final_labels
    
    def _analyze_chunk(self, chunk: pd.DataFrame) -> Dict:
        """åˆ†æå–®å€‹æ•¸æ“šå¡Š"""
        stats = {
            'samples': len(chunk),
            'sign_languages': chunk['sign_language'].value_counts().to_dict(),
            'missing_rates': chunk.isnull().sum() / len(chunk)
        }
        return stats
    
    def _merge_chunk_stats(self, chunk_stats: List[Dict]) -> Dict:
        """åˆä½µå¡Šçµ±è¨ˆä¿¡æ¯"""
        # å¯¦ç¾çµ±è¨ˆä¿¡æ¯åˆä½µé‚è¼¯
        merged = {
            'samples': sum(s['samples'] for s in chunk_stats),
            'sign_languages': {},
            'missing_rates': {}
        }
        # è©³ç´°å¯¦ç¾...
        return merged
    
    def _merge_file_stats(self, all_stats: Dict, file_stats: Dict) -> Dict:
        """åˆä½µæª”æ¡ˆçµ±è¨ˆä¿¡æ¯"""
        # å¯¦ç¾æª”æ¡ˆé–“çµ±è¨ˆåˆä½µ
        return all_stats
    
    def _pose_constrained_interpolation(self, df: pd.DataFrame) -> pd.DataFrame:
        """åŸºæ–¼èº«é«”å§¿æ…‹ç´„æŸçš„æ’å€¼"""
        # å¯¦ç¾åŸºæ–¼èº«é«”çµæ§‹çš„ç´„æŸæ’å€¼
        return df
    
    def _fill_remaining_missing(self, df: pd.DataFrame) -> pd.DataFrame:
        """å¡«å……å‰©é¤˜ç¼ºå¤±å€¼"""
        # ä½¿ç”¨èº«é«”ä¸­å¿ƒé»æˆ–çµ±è¨ˆå€¼å¡«å……
        return df
    
    def _time_warp(self, sequences: np.ndarray) -> np.ndarray:
        """æ™‚é–“æ‰­æ›²å¢å¼·"""
        # å¯¦ç¾æ™‚é–“è»¸è®Šæ›
        return sequences
    
    def _spatial_transform(self, sequences: np.ndarray) -> np.ndarray:
        """ç©ºé–“è®Šæ›å¢å¼·"""
        # å¯¦ç¾æ—‹è½‰ã€ç¸®æ”¾è®Šæ›
        return sequences
    
    def _add_noise(self, sequences: np.ndarray) -> np.ndarray:
        """æ·»åŠ å™ªè²å¢å¼·"""
        # å¯¦ç¾é«˜æ–¯å™ªè²æ·»åŠ 
        noise_scale = 0.01
        noise = np.random.normal(0, noise_scale, sequences.shape)
        return sequences + noise

def main():
    """ä¸»å‡½æ•¸ - æ¼”ç¤ºä½¿ç”¨æ–¹æ³•"""
    print("ğŸš€ æ‰‹èªè¾¨è­˜æ•¸æ“šé è™•ç†æµç¨‹")
    print("=" * 50)
    
    # åˆå§‹åŒ–é è™•ç†å™¨
    preprocessor = SignLanguagePreprocessor(sequence_length=30, stride=15)
    
    # æ•¸æ“šæª”æ¡ˆåˆ—è¡¨
    csv_files = [
        'dataset/sign_language1.csv',
        'dataset/sign_language2.csv',
        # ... å…¶ä»–æª”æ¡ˆ
    ]
    
    # æ•¸æ“šåˆ†æ
    stats = preprocessor.load_and_analyze_data(csv_files)
    print(f"ğŸ“Š ç¸½æ¨£æœ¬æ•¸: {stats['total_samples']}")
    
    # è™•ç†å–®å€‹æª”æ¡ˆç¤ºä¾‹
    df = pd.read_csv('dataset/sign_language1.csv')
    print(f"ğŸ“„ è¼‰å…¥æª”æ¡ˆï¼Œå½¢ç‹€: {df.shape}")
    
    # ç¼ºå¤±å€¼è™•ç†
    df_clean = preprocessor.handle_missing_values(df)
    print(f"ğŸ”§ ç¼ºå¤±å€¼è™•ç†å®Œæˆ")
    
    # åº§æ¨™æ¨™æº–åŒ–
    df_normalized = preprocessor.normalize_coordinates(df_clean)
    print(f"ğŸ“ åº§æ¨™æ¨™æº–åŒ–å®Œæˆ")
    
    # ç”Ÿæˆåºåˆ—
    sequences, labels = preprocessor.create_sequences(df_normalized)
    print(f"ğŸ”„ åºåˆ—ç”Ÿæˆå®Œæˆ: {sequences.shape}")
    
    # æ•¸æ“šå¢å¼·
    aug_sequences, aug_labels = preprocessor.augment_sequences(sequences, labels)
    print(f"ğŸ­ æ•¸æ“šå¢å¼·å®Œæˆ: {aug_sequences.shape}")
    
    print("âœ… é è™•ç†æµç¨‹å®Œæˆ!")

if __name__ == "__main__":
    main()
