"""
å¿«é€Ÿé©—è­‰è…³æœ¬ï¼šæ¸¬è©¦æ•¸æ“šé è™•ç†ç®¡é“
åœ¨é–‹å§‹å¤§è¦æ¨¡è¨“ç·´å‰ï¼Œç¢ºä¿æ‰€æœ‰çµ„ä»¶æ­£å¸¸å·¥ä½œ
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from src.data_preprocessing import SignLanguagePreprocessor
import time

def quick_data_verification():
    """å¿«é€Ÿæ•¸æ“šé©—è­‰"""
    print("ğŸ” å¿«é€Ÿæ•¸æ“šé©—è­‰é–‹å§‹")
    print("=" * 50)
    
    # è¼‰å…¥ä¸€å€‹å°æ¨£æœ¬é€²è¡Œæ¸¬è©¦
    sample_file = "dataset/sign_language1.csv"
    
    if not os.path.exists(sample_file):
        print(f"âŒ æ‰¾ä¸åˆ°æª”æ¡ˆ: {sample_file}")
        return False
    
    print(f"ğŸ“„ è¼‰å…¥æ¸¬è©¦æª”æ¡ˆ: {sample_file}")
    
    # è®€å–å‰1000è¡Œé€²è¡Œå¿«é€Ÿæ¸¬è©¦
    df_sample = pd.read_csv(sample_file, nrows=1000)
    print(f"ğŸ“Š æ¨£æœ¬å½¢ç‹€: {df_sample.shape}")
    print(f"ğŸ·ï¸ æ‰‹èªé¡åˆ¥: {df_sample['sign_language'].unique()}")
    
    # æª¢æŸ¥åŸºæœ¬çµæ§‹
    expected_columns = ['sign_language', 'source_video', 'frame']
    for col in expected_columns:
        if col not in df_sample.columns:
            print(f"âŒ ç¼ºå°‘å¿…è¦æ¬„ä½: {col}")
            return False
    
    print("âœ… åŸºæœ¬çµæ§‹æª¢æŸ¥é€šé")
    
    # ç¼ºå¤±å€¼åˆ†æ
    missing_rates = df_sample.isnull().sum() / len(df_sample) * 100
    high_missing = missing_rates[missing_rates > 50]
    
    print(f"ğŸ“ˆ é«˜ç¼ºå¤±ç‡æ¬„ä½ (>50%): {len(high_missing)}")
    if len(high_missing) > 0:
        print("   ä¸»è¦ç¼ºå¤±æ¬„ä½:", high_missing.head().to_dict())
    
    return True

def test_preprocessing_pipeline():
    """æ¸¬è©¦é è™•ç†ç®¡é“"""
    print("\nğŸ§ª æ¸¬è©¦é è™•ç†ç®¡é“")
    print("=" * 50)
    
    try:
        # åˆå§‹åŒ–é è™•ç†å™¨
        preprocessor = SignLanguagePreprocessor(sequence_length=30, stride=15)
        print("âœ… é è™•ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
        
        # è¼‰å…¥æ¸¬è©¦æ•¸æ“š
        df_sample = pd.read_csv("dataset/sign_language1.csv", nrows=1000)
        print(f"ğŸ“„ è¼‰å…¥æ¸¬è©¦æ•¸æ“š: {df_sample.shape}")
        
        # æ¸¬è©¦ç¼ºå¤±å€¼è™•ç†
        print("ğŸ”§ æ¸¬è©¦ç¼ºå¤±å€¼è™•ç†...")
        start_time = time.time()
        df_clean = preprocessor.handle_missing_values(df_sample)
        process_time = time.time() - start_time
        print(f"   è™•ç†æ™‚é–“: {process_time:.2f}ç§’")
        print(f"   ç¼ºå¤±å€¼æ¸›å°‘: {df_sample.isnull().sum().sum()} â†’ {df_clean.isnull().sum().sum()}")
        
        # æ¸¬è©¦åº§æ¨™æ¨™æº–åŒ–
        print("ğŸ“ æ¸¬è©¦åº§æ¨™æ¨™æº–åŒ–...")
        start_time = time.time()
        df_normalized = preprocessor.normalize_coordinates(df_clean)
        process_time = time.time() - start_time
        print(f"   è™•ç†æ™‚é–“: {process_time:.2f}ç§’")
        
        # æ¸¬è©¦åºåˆ—ç”Ÿæˆ
        print("ğŸ”„ æ¸¬è©¦åºåˆ—ç”Ÿæˆ...")
        start_time = time.time()
        sequences, labels = preprocessor.create_sequences(df_normalized)
        process_time = time.time() - start_time
        print(f"   è™•ç†æ™‚é–“: {process_time:.2f}ç§’")
        print(f"   ç”Ÿæˆåºåˆ—æ•¸é‡: {len(sequences)}")
        print(f"   åºåˆ—å½¢ç‹€: {sequences.shape if len(sequences) > 0 else 'None'}")
        print(f"   æ¨™ç±¤ç¨®é¡: {len(np.unique(labels)) if len(labels) > 0 else 0}")
        
        if len(sequences) > 0:
            # æ¸¬è©¦æ•¸æ“šå¢å¼·
            print("ğŸ­ æ¸¬è©¦æ•¸æ“šå¢å¼·...")
            start_time = time.time()
            aug_sequences, aug_labels = preprocessor.augment_sequences(
                sequences[:10], labels[:10], augment_factor=2.0  # åªæ¸¬è©¦å‰10å€‹åºåˆ—
            )
            process_time = time.time() - start_time
            print(f"   è™•ç†æ™‚é–“: {process_time:.2f}ç§’")
            print(f"   å¢å¼·å¾Œæ•¸é‡: {len(aug_sequences)}")
            
            return True
        else:
            print("âŒ æœªèƒ½ç”Ÿæˆæœ‰æ•ˆåºåˆ—")
            return False
            
    except Exception as e:
        print(f"âŒ é è™•ç†ç®¡é“æ¸¬è©¦å¤±æ•—: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def analyze_sequence_characteristics():
    """åˆ†æåºåˆ—ç‰¹å¾µ"""
    print("\nğŸ“Š åºåˆ—ç‰¹å¾µåˆ†æ")
    print("=" * 50)
    
    try:
        # è¼‰å…¥ä¸¦è™•ç†æ•¸æ“š
        preprocessor = SignLanguagePreprocessor(sequence_length=30, stride=15)
        df_sample = pd.read_csv("dataset/sign_language1.csv", nrows=5000)  # æ›´å¤§æ¨£æœ¬
        
        print("è™•ç†æ•¸æ“šä¸­...")
        df_clean = preprocessor.handle_missing_values(df_sample)
        df_normalized = preprocessor.normalize_coordinates(df_clean)
        sequences, labels = preprocessor.create_sequences(df_normalized)
        
        if len(sequences) == 0:
            print("âŒ ç„¡æ³•ç”Ÿæˆåºåˆ—é€²è¡Œåˆ†æ")
            return
        
        print(f"ğŸ“ˆ åˆ†æ {len(sequences)} å€‹åºåˆ—")
        
        # åŸºæœ¬çµ±è¨ˆ
        print(f"   åºåˆ—å½¢ç‹€: {sequences.shape}")
        print(f"   ç‰¹å¾µç¶­åº¦: {sequences.shape[2]}")
        print(f"   æ•¸å€¼ç¯„åœ: [{np.nanmin(sequences):.3f}, {np.nanmax(sequences):.3f}]")
        print(f"   å¹³å‡å€¼: {np.nanmean(sequences):.3f}")
        print(f"   æ¨™æº–å·®: {np.nanstd(sequences):.3f}")
        
        # æ¨™ç±¤åˆ†å¸ƒ
        unique_labels, counts = np.unique(labels, return_counts=True)
        print(f"   æ¨™ç±¤åˆ†å¸ƒ:")
        for label, count in zip(unique_labels, counts):
            print(f"     {label}: {count} å€‹åºåˆ—")
        
        # ç¼ºå¤±å€¼åˆ†æ
        missing_ratio = np.isnan(sequences).sum() / sequences.size
        print(f"   ç¼ºå¤±å€¼æ¯”ä¾‹: {missing_ratio:.1%}")
        
        return sequences, labels
        
    except Exception as e:
        print(f"âŒ åºåˆ—åˆ†æå¤±æ•—: {str(e)}")
        return None, None

def performance_benchmark():
    """æ€§èƒ½åŸºæº–æ¸¬è©¦"""
    print("\nâš¡ æ€§èƒ½åŸºæº–æ¸¬è©¦")
    print("=" * 50)
    
    # æ¸¬è©¦ä¸åŒæ•¸æ“šé‡çš„è™•ç†æ™‚é–“
    test_sizes = [100, 500, 1000, 5000]
    
    for size in test_sizes:
        try:
            print(f"ğŸ“Š æ¸¬è©¦ {size} æ¨£æœ¬...")
            
            start_time = time.time()
            
            # è¼‰å…¥æ•¸æ“š
            df = pd.read_csv("dataset/sign_language1.csv", nrows=size)
            load_time = time.time() - start_time
            
            # é è™•ç†
            preprocessor = SignLanguagePreprocessor()
            
            process_start = time.time()
            df_clean = preprocessor.handle_missing_values(df)
            df_normalized = preprocessor.normalize_coordinates(df_clean)
            sequences, labels = preprocessor.create_sequences(df_normalized)
            process_time = time.time() - process_start
            
            total_time = time.time() - start_time
            
            print(f"   è¼‰å…¥æ™‚é–“: {load_time:.2f}s")
            print(f"   è™•ç†æ™‚é–“: {process_time:.2f}s")  
            print(f"   ç¸½æ™‚é–“: {total_time:.2f}s")
            print(f"   ç”Ÿæˆåºåˆ—: {len(sequences)}")
            print(f"   è™•ç†é€Ÿåº¦: {size/total_time:.1f} æ¨£æœ¬/ç§’")
            print()
            
        except Exception as e:
            print(f"   âŒ {size} æ¨£æœ¬æ¸¬è©¦å¤±æ•—: {str(e)}")

def estimate_full_processing_time():
    """ä¼°ç®—å®Œæ•´æ•¸æ“šè™•ç†æ™‚é–“"""
    print("\nâ±ï¸ å®Œæ•´æ•¸æ“šè™•ç†æ™‚é–“ä¼°ç®—")
    print("=" * 50)
    
    # åŸºæ–¼å°æ¨£æœ¬ä¼°ç®—
    sample_size = 1000
    total_samples = 1_210_017  # åŸºæ–¼æ•¸æ“šåˆ†æçµæœ
    
    try:
        start_time = time.time()
        
        df = pd.read_csv("dataset/sign_language1.csv", nrows=sample_size)
        preprocessor = SignLanguagePreprocessor()
        
        df_clean = preprocessor.handle_missing_values(df)
        df_normalized = preprocessor.normalize_coordinates(df_clean)
        sequences, labels = preprocessor.create_sequences(df_normalized)
        
        sample_time = time.time() - start_time
        
        # ä¼°ç®—ç¸½æ™‚é–“
        estimated_total_hours = (sample_time / sample_size) * total_samples / 3600
        
        print(f"ğŸ“Š æ¨£æœ¬å¤§å°: {sample_size}")
        print(f"   è™•ç†æ™‚é–“: {sample_time:.2f}ç§’")
        print(f"   è™•ç†é€Ÿåº¦: {sample_size/sample_time:.1f} æ¨£æœ¬/ç§’")
        print()
        print(f"ğŸ”® å®Œæ•´æ•¸æ“šä¼°ç®—:")
        print(f"   ç¸½æ¨£æœ¬æ•¸: {total_samples:,}")
        print(f"   é ä¼°ç¸½æ™‚é–“: {estimated_total_hours:.1f} å°æ™‚")
        print(f"   å»ºè­°: åˆ†æ‰¹è™•ç†ï¼Œä½¿ç”¨å¤šé€²ç¨‹")
        
        if estimated_total_hours > 12:
            print("âš ï¸  è­¦å‘Š: è™•ç†æ™‚é–“éé•·ï¼Œå»ºè­°:")
            print("   1. å¢åŠ è™•ç†å™¨æ ¸å¿ƒæ•¸")
            print("   2. å„ªåŒ–ç®—æ³•æ•ˆç‡") 
            print("   3. åˆ†æ‰¹ä¸¦è¡Œè™•ç†")
            
    except Exception as e:
        print(f"âŒ ä¼°ç®—å¤±æ•—: {str(e)}")

def main():
    """ä¸»é©—è­‰æµç¨‹"""
    print("ğŸš€ æ‰‹èªæ•¸æ“šé è™•ç†é©—è­‰æµç¨‹")
    print("ğŸ• é–‹å§‹æ™‚é–“:", time.strftime("%Y-%m-%d %H:%M:%S"))
    print("=" * 60)
    
    # 1. åŸºæœ¬æ•¸æ“šé©—è­‰
    if not quick_data_verification():
        print("âŒ åŸºæœ¬é©—è­‰å¤±æ•—ï¼Œè«‹æª¢æŸ¥æ•¸æ“šæª”æ¡ˆ")
        return
    
    # 2. é è™•ç†ç®¡é“æ¸¬è©¦
    if not test_preprocessing_pipeline():
        print("âŒ é è™•ç†ç®¡é“æ¸¬è©¦å¤±æ•—")
        return
    
    # 3. åºåˆ—ç‰¹å¾µåˆ†æ
    sequences, labels = analyze_sequence_characteristics()
    
    # 4. æ€§èƒ½åŸºæº–æ¸¬è©¦
    performance_benchmark()
    
    # 5. å®Œæ•´è™•ç†æ™‚é–“ä¼°ç®—
    estimate_full_processing_time()
    
    print("\n" + "=" * 60)
    print("âœ… é©—è­‰æµç¨‹å®Œæˆ!")
    print("ğŸ• çµæŸæ™‚é–“:", time.strftime("%Y-%m-%d %H:%M:%S"))
    
    # ç¸½çµå»ºè­°
    print("\nğŸ“‹ ä¸‹ä¸€æ­¥å»ºè­°:")
    print("1. å¦‚æœé©—è­‰é€šéï¼Œé–‹å§‹å®Œæ•´æ•¸æ“šé è™•ç†")
    print("2. å¯¦ç¾åŸºç¤GRUæ¨¡å‹")
    print("3. é–‹å§‹è¨“ç·´å¯¦é©—")

if __name__ == "__main__":
    main()
